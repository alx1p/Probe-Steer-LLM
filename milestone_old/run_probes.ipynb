{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import HuggingfaceModel\n",
    "from simpleqa_eval import SimpleQAEvalMod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the SQUAD 2.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the SQuAD 2.0 dataset\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Convert the training and validation splits to pandas DataFrames\n",
    "train_df = pd.DataFrame(squad_dataset['train'])\n",
    "validation_df = pd.DataFrame(squad_dataset['validation'])\n",
    "\n",
    "# Make a base prompt column\n",
    "train_df[\"base_prompt\"] = train_df.apply(\n",
    "    lambda row: f\"Answer the following question as briefly as possible.\\nContext: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "validation_df[\"base_prompt\"] = validation_df.apply(\n",
    "    lambda row: f\"Answer the following question as briefly as possible.\\nContext: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check for impossible samples in the dataset (no answer)\n",
    "train_df['is_impossible'] = train_df['answers'].apply(lambda x: len(x['text']) == 0)\n",
    "validation_df['is_impossible'] = validation_df['answers'].apply(lambda x: len(x['text']) == 0)\n",
    "\n",
    "# Separate possible and impossible samples\n",
    "possible_samples_train = train_df[~train_df['is_impossible']]\n",
    "impossible_samples_train = train_df[train_df['is_impossible']]\n",
    "possible_samples_test = validation_df[~validation_df['is_impossible']]\n",
    "impossible_samples_test = validation_df[validation_df['is_impossible']]\n",
    "\n",
    "# Randomly sample 500 rows from possible and impossible test samples\n",
    "sampled_possible = possible_samples_test.sample(n=500, random_state=42)\n",
    "sampled_impossible = impossible_samples_test.sample(n=500, random_state=42)\n",
    "\n",
    "# Add a column indicating the type of sample\n",
    "sampled_possible['type'] = 'possible'\n",
    "sampled_impossible['type'] = 'impossible'\n",
    "\n",
    "# Concatenate the sampled datasets into one\n",
    "combined_samples = pd.concat([sampled_possible, sampled_impossible], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset to mix possible and impossible samples\n",
    "combined_samples = combined_samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Function to generate a few-shot prompt dynamically\n",
    "def generate_few_shot_prompt(row):\n",
    "    # Randomly sample one example from possible and impossible train sets\n",
    "    example_possible = possible_samples_train.sample(n=1).iloc[0]\n",
    "    example_impossible = impossible_samples_train.sample(n=1).iloc[0]\n",
    "    \n",
    "    # Construct the few-shot examples\n",
    "    example_1 = (\n",
    "        f\"Example 1: Answer the following question as briefly as possible.\\n\"\n",
    "        f\"Context: {example_possible['context']}\\n\"\n",
    "        f\"Question: {example_possible['question']}\\n\"\n",
    "        f\"Answer: {example_possible['answers']['text'][0] if example_possible['answers']['text'] else ''}\"\n",
    "    )\n",
    "    \n",
    "    example_2 = (\n",
    "        f\"Example 2: Answer the following question as briefly as possible.\\n\"\n",
    "        f\"Context: {example_impossible['context']}\\n\"\n",
    "        f\"Question: {example_impossible['question']}\\n\"\n",
    "        f\"Answer: {example_impossible['answers']['text'][0] if example_impossible['answers']['text'] else ''}\"\n",
    "    )\n",
    "    \n",
    "    # Combine with the current row's prompt\n",
    "    return (\n",
    "        f\"{example_1}\\n\\n\"\n",
    "        f\"{example_2}\\n\\n\"\n",
    "        f\"Answer the following question as briefly as possible.\\n\"\n",
    "        f\"Context: {row['context']}\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "# Apply the function to create the \"few_shot_prompt\" column\n",
    "combined_samples['few_shot_prompt'] = combined_samples.apply(generate_few_shot_prompt, axis=1)\n",
    "\n",
    "# Display a sample of the new column\n",
    "combined_samples[['few_shot_prompt']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load HF model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "cache_dir = \"./\"\n",
    "hf_model = HuggingfaceModel(model_name, cache_dir, max_new_tokens=200)\n",
    "\n",
    "# simple qa eval\n",
    "client = OpenAI() # key taken away\n",
    "simpleqa_evaluator = SimpleQAEvalMod(client = client, model='gpt-4o', grader_template=GRADER_TEMPLATE)\n",
    "\n",
    "\n",
    "model_dict_slt_ent = joblib.load('model_dict_slt_ent.pkl')\n",
    "model_dict_tbg_ent = joblib.load('model_dict_tbg_ent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end flow\n",
    "\n",
    "Below, for each row the generated squad dataset, we run the prompts through the model, get the answer and hidden states, and then run the hidden states for the TBG and SLT tokens through the loaded SEP probes to get entropy probabilities and predictions. All results get stored every 20 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Process rows in chunks of 20\n",
    "chunk_size = 20\n",
    "for start_index in range(0, len(combined_samples), chunk_size):\n",
    "    end_index = min(start_index + chunk_size, len(combined_samples))\n",
    "    chunk = combined_samples.iloc[start_index:end_index]\n",
    "\n",
    "    # Initialize lists for this chunk\n",
    "    base_columns = []\n",
    "    fs_columns = []\n",
    "\n",
    "    for index, row in chunk.iterrows():\n",
    "        print(index)\n",
    "        real_answer = row['answers']['text'][0] if row['answers']['text'] else ''\n",
    "        if real_answer=='' or row['answers']['text']==[]:\n",
    "            real_answer = \"The answer is not found in the context.\"\n",
    "        \n",
    "        # Generate model predictions for base prompts\n",
    "        base_output_text, base_hidden_states = hf_model.predict(row['base_prompt'], temperature=1.0, return_latent=True)\n",
    "        base_answer = base_output_text[len(row['base_prompt']):].strip()\n",
    "        model_probs_preds_base = {\"base_answer\": base_answer}\n",
    "\n",
    "        oai_grade_base = simpleqa_evaluator.grade_sample(\n",
    "            question=row['base_prompt'], target=real_answer, predicted_answer=base_answer\n",
    "        )\n",
    "        model_probs_preds_base[\"simple_qa_base_result\"] = oai_grade_base\n",
    "\n",
    "        sec_last_token_embedding = base_hidden_states[1]\n",
    "        last_tok_bef_gen_embedding = base_hidden_states[-1]\n",
    "\n",
    "        for layer_num in range(sec_last_token_embedding.shape[0]):\n",
    "            # Process second last token\n",
    "            slt_high_ent_prob = model_dict_slt_ent[layer_num].predict_proba(\n",
    "                [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "            )[0][1]\n",
    "            slt_high_ent_pred = model_dict_slt_ent[layer_num].predict(\n",
    "                [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "            )[0]\n",
    "            model_probs_preds_base[f\"base_slt_layer_{layer_num}_prob\"] = slt_high_ent_prob\n",
    "            model_probs_preds_base[f\"base_slt_layer_{layer_num}_pred\"] = slt_high_ent_pred\n",
    "\n",
    "            # Process last token before generation\n",
    "            tbg_high_ent_prob = model_dict_tbg_ent[layer_num].predict_proba(\n",
    "                [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "            )[0][1]\n",
    "            tbg_high_ent_pred = model_dict_tbg_ent[layer_num].predict(\n",
    "                [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "            )[0]\n",
    "            model_probs_preds_base[f\"base_tbg_layer_{layer_num}_prob\"] = tbg_high_ent_prob\n",
    "            model_probs_preds_base[f\"base_tbg_layer_{layer_num}_pred\"] = tbg_high_ent_pred\n",
    "\n",
    "        # Store base columns\n",
    "        base_columns.append(model_probs_preds_base)\n",
    "\n",
    "        # Generate model predictions for few-shot prompts\n",
    "        fs_output_text, fs_hidden_states = hf_model.predict(row['few_shot_prompt'], temperature=1.0, return_latent=True)\n",
    "        fs_answer = fs_output_text[len(row['few_shot_prompt']):].split(\"\\n\")[0].strip()\n",
    "        model_probs_preds_fs = {\"fs_answer\": fs_answer}\n",
    "\n",
    "        oai_grade_fs = simpleqa_evaluator.grade_sample(\n",
    "            question=row['base_prompt'], target=real_answer, predicted_answer=fs_answer\n",
    "        )\n",
    "        model_probs_preds_fs[\"simple_qa_fs_result\"] = oai_grade_fs\n",
    "\n",
    "        sec_last_token_embedding = fs_hidden_states[1]\n",
    "        last_tok_bef_gen_embedding = fs_hidden_states[-1]\n",
    "\n",
    "        for layer_num in range(sec_last_token_embedding.shape[0]):\n",
    "            # Process second last token\n",
    "            slt_high_ent_prob = model_dict_slt_ent[layer_num].predict_proba(\n",
    "                [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "            )[0][1]\n",
    "            slt_high_ent_pred = model_dict_slt_ent[layer_num].predict(\n",
    "                [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "            )[0]\n",
    "            model_probs_preds_fs[f\"fs_slt_layer_{layer_num}_prob\"] = slt_high_ent_prob\n",
    "            model_probs_preds_fs[f\"fs_slt_layer_{layer_num}_pred\"] = slt_high_ent_pred\n",
    "\n",
    "            # Process last token before generation\n",
    "            tbg_high_ent_prob = model_dict_tbg_ent[layer_num].predict_proba(\n",
    "                [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "            )[0][1]\n",
    "            tbg_high_ent_pred = model_dict_tbg_ent[layer_num].predict(\n",
    "                [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "            )[0]\n",
    "            model_probs_preds_fs[f\"fs_tbg_layer_{layer_num}_prob\"] = tbg_high_ent_prob\n",
    "            model_probs_preds_fs[f\"fs_tbg_layer_{layer_num}_pred\"] = tbg_high_ent_pred\n",
    "\n",
    "        # Store few-shot columns\n",
    "        fs_columns.append(model_probs_preds_fs)\n",
    "\n",
    "    # Convert lists of dictionaries into DataFrames for this chunk\n",
    "    base_df = pd.DataFrame(base_columns)\n",
    "    fs_df = pd.DataFrame(fs_columns)\n",
    "\n",
    "    # Concatenate the new columns with the current chunk\n",
    "    chunk_with_predictions = pd.concat([chunk.reset_index(drop=True), base_df, fs_df], axis=1)\n",
    "\n",
    "    # Save the chunk with an identifier\n",
    "    chunk_identifier = f\"{start_index}_{end_index - 1}\"\n",
    "    filename = f\"processed_chunk_{chunk_identifier}.csv\"\n",
    "    chunk_with_predictions.to_csv(filename, index=False)\n",
    "    print(f\"Saved chunk: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
