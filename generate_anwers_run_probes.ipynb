{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating LLM Answers via Steering and Applying SEPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates how we applied the SEPs and applied the steering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader import HuggingfaceModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Squad 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD 2.0 dataset\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Convert the training and validation splits to pandas DataFrames\n",
    "train_df = pd.DataFrame(squad_dataset['train'])\n",
    "validation_df = pd.DataFrame(squad_dataset['validation'])\n",
    "\n",
    "# Make a base prompt column\n",
    "train_df[\"base_prompt\"] = train_df.apply(\n",
    "    lambda row: f\"Answer the following question using only the context provided. If you cannot answer it given the context provided, then you can refuse to answer the question. For example, if the answer is not clear from the context, you should refuse. You must NOT use knowledge outside the context provided to answer the question.\\nContext: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "validation_df[\"base_prompt\"] = validation_df.apply(\n",
    "    lambda row: f\"Answer the following question using only the context provided. If you cannot answer it given the context provided, then you can refuse to answer the question. For example, if the answer is not clear from the context, you should refuse. You must NOT use knowledge outside the context provided to answer the question.\\nContext: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check for impossible samples in the dataset (no answer)\n",
    "train_df['is_impossible'] = train_df['answers'].apply(lambda x: len(x['text']) == 0)\n",
    "validation_df['is_impossible'] = validation_df['answers'].apply(lambda x: len(x['text']) == 0)\n",
    "\n",
    "### temp\n",
    "# validation_df = validation_df.sample(frac=0.2, random_state=42)\n",
    "####\n",
    "\n",
    "# Split the validation set into a new train and test set (70% train, 30% test)\n",
    "train_val_split = validation_df.sample(frac=0.7, random_state=42)\n",
    "test_val_split = validation_df.drop(train_val_split.index)\n",
    "\n",
    "# Separate possible and impossible samples from the new training and test sets\n",
    "possible_samples_train = train_val_split[~train_val_split['is_impossible']]\n",
    "impossible_samples_train = train_val_split[train_val_split['is_impossible']]\n",
    "possible_samples_test = test_val_split[~test_val_split['is_impossible']]\n",
    "impossible_samples_test = test_val_split[test_val_split['is_impossible']]\n",
    "\n",
    "\n",
    "# Add a column indicating the type of sample\n",
    "possible_samples_train['type'] = 'possible'\n",
    "possible_samples_test['type'] = 'possible'\n",
    "impossible_samples_train['type'] = 'impossible'\n",
    "impossible_samples_test['type'] = 'impossible'\n",
    "\n",
    "# Concatenate the sampled datasets into one\n",
    "combined_samples_train = pd.concat([possible_samples_train, impossible_samples_train], ignore_index=True)\n",
    "combined_samples_test = pd.concat([possible_samples_test, impossible_samples_test], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset to mix possible and impossible samples\n",
    "combined_samples_train = combined_samples_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "combined_samples_test = combined_samples_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_samples_train.to_csv(\"combined_samples_train_new.csv\")\n",
    "combined_samples_test.to_csv(\"combined_samples_test_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Models and Steering Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper to Apply Activation Steering\n",
    "\n",
    "This code, adopted from https://github.com/nrimsky/CAA allow us to apply steering vectors. Please download the repository so you can directly call CAA as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from CAA.utils.helpers import add_vector_from_position, find_instruction_end_postion, get_model_path\n",
    "from CAA.utils.tokenize import (\n",
    "    tokenize_llama_chat,\n",
    "    tokenize_llama_base,\n",
    "    ADD_FROM_POS_BASE,\n",
    "    ADD_FROM_POS_CHAT,\n",
    ")\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class AttnWrapper(t.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for attention mechanism to save activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.activations = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.attn(*args, **kwargs)\n",
    "        self.activations = output[0]\n",
    "        return output\n",
    "\n",
    "\n",
    "class BlockOutputWrapper(t.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for block to save activations and unembed them\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, unembed_matrix, norm, tokenizer):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
    "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
    "\n",
    "        self.attn_out_unembedded = None\n",
    "        self.intermediate_resid_unembedded = None\n",
    "        self.mlp_out_unembedded = None\n",
    "        self.block_out_unembedded = None\n",
    "\n",
    "        self.activations = None\n",
    "        self.add_activations = None\n",
    "        self.from_position = None\n",
    "\n",
    "        self.save_internal_decodings = False\n",
    "\n",
    "        self.calc_dot_product_with = None\n",
    "        self.dot_products = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.activations = output[0]\n",
    "        if self.calc_dot_product_with is not None:\n",
    "            last_token_activations = self.activations[0, -1, :]\n",
    "            decoded_activations = self.unembed_matrix(self.norm(last_token_activations))\n",
    "            top_token_id = t.topk(decoded_activations, 1)[1][0]\n",
    "            top_token = self.tokenizer.decode(top_token_id)\n",
    "            dot_product = t.dot(last_token_activations, self.calc_dot_product_with) / (\n",
    "                t.norm(last_token_activations) * t.norm(self.calc_dot_product_with)\n",
    "            )\n",
    "            self.dot_products.append((top_token, dot_product.cpu().item()))\n",
    "        if self.add_activations is not None:\n",
    "            augmented_output = add_vector_from_position(\n",
    "                matrix=output[0],\n",
    "                vector=self.add_activations,\n",
    "                position_ids=kwargs[\"position_ids\"],\n",
    "                from_pos=self.from_position,\n",
    "            )\n",
    "            output = (augmented_output,) + output[1:]\n",
    "\n",
    "        if not self.save_internal_decodings:\n",
    "            return output\n",
    "\n",
    "        # Whole block unembedded\n",
    "        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "\n",
    "        # Self-attention unembedded\n",
    "        attn_output = self.block.self_attn.activations\n",
    "        self.attn_out_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "\n",
    "        # Intermediate residual unembedded\n",
    "        attn_output += args[0]\n",
    "        self.intermediate_resid_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "\n",
    "        # MLP unembedded\n",
    "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
    "        self.mlp_out_unembedded = self.unembed_matrix(self.norm(mlp_output))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def add(self, activations):\n",
    "        self.add_activations = activations\n",
    "\n",
    "    def reset(self):\n",
    "        self.add_activations = None\n",
    "        self.activations = None\n",
    "        self.block.self_attn.activations = None\n",
    "        self.from_position = None\n",
    "        self.calc_dot_product_with = None\n",
    "        self.dot_products = []\n",
    "\n",
    "\n",
    "import torch as t\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "class LlamaWrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: str = \"7b\",\n",
    "        model_path = \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        use_chat: bool = True,\n",
    "        override_model_weights_path: Optional[str] = None,\n",
    "        cache_dir: Optional[str] = \"./\",\n",
    "        gpu_device = 1,  # Default to device 1\n",
    "    ):\n",
    "        # self.device = gpu_device if t.cuda.is_available() else \"cpu\"\n",
    "        self.device = \"cuda:1\"\n",
    "        self.use_chat = use_chat\n",
    "        self.model_name_path = model_path\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name_path,\n",
    "            cache_dir=cache_dir,\n",
    "            use_fast=False,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_path,\n",
    "            cache_dir=cache_dir,\n",
    "            device_map={\"\": gpu_device},  # Use specified device\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Override weights if specified\n",
    "        if override_model_weights_path is not None:\n",
    "            self.model.load_state_dict(t.load(override_model_weights_path))\n",
    "        \n",
    "        # Convert model to half-precision for non-7b models\n",
    "        if size != \"7b\":\n",
    "            self.model = self.model.half()\n",
    "\n",
    "        # Move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Set end-of-sequence token\n",
    "        if use_chat:\n",
    "            self.END_STR = t.tensor(self.tokenizer.encode(ADD_FROM_POS_CHAT)[1:]).to(\n",
    "                self.device\n",
    "            )\n",
    "        else:\n",
    "            self.END_STR = t.tensor(self.tokenizer.encode(ADD_FROM_POS_BASE)[1:]).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "        # Wrap layers with BlockOutputWrapper\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            self.model.model.layers[i] = BlockOutputWrapper(\n",
    "                layer, self.model.lm_head, self.model.model.norm, self.tokenizer\n",
    "            )\n",
    "\n",
    "    def set_save_internal_decodings(self, value: bool):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.save_internal_decodings = value\n",
    "\n",
    "    def set_from_positions(self, pos: int):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.from_position = pos\n",
    "\n",
    "    def generate(self, tokens, max_new_tokens=100):\n",
    "        with t.no_grad():\n",
    "            instr_pos = find_instruction_end_postion(tokens[0], self.END_STR)\n",
    "            self.set_from_positions(instr_pos)\n",
    "            generated = self.model.generate(\n",
    "                inputs=tokens, max_new_tokens=max_new_tokens, top_k=1, temperature=1.0\n",
    "            )\n",
    "            return self.tokenizer.batch_decode(generated)[0]\n",
    "\n",
    "    def generate_text(self, user_input: str, model_output: Optional[str] = None, system_prompt: Optional[str] = None, max_new_tokens: int = 50) -> str:\n",
    "        if self.use_chat:\n",
    "            tokens = tokenize_llama_chat(\n",
    "                tokenizer=self.tokenizer, user_input=user_input, model_output=model_output, system_prompt=system_prompt\n",
    "            )\n",
    "        else:\n",
    "            tokens = tokenize_llama_base(tokenizer=self.tokenizer, user_input=user_input, model_output=model_output)\n",
    "        tokens = t.tensor(tokens).unsqueeze(0).to(self.device)\n",
    "        return self.generate(tokens, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    def get_logits(self, tokens):\n",
    "        with t.no_grad():\n",
    "            instr_pos = find_instruction_end_postion(tokens[0], self.END_STR)\n",
    "            self.set_from_positions(instr_pos)\n",
    "            logits = self.model(tokens).logits\n",
    "            return logits\n",
    "\n",
    "    def get_logits_from_text(self, user_input: str, model_output: Optional[str] = None, system_prompt: Optional[str] = None) -> t.Tensor:\n",
    "        if self.use_chat:\n",
    "            tokens = tokenize_llama_chat(\n",
    "                tokenizer=self.tokenizer, user_input=user_input, model_output=model_output, system_prompt=system_prompt\n",
    "            )\n",
    "        else:\n",
    "            tokens = tokenize_llama_base(tokenizer=self.tokenizer, user_input=user_input, model_output=model_output)\n",
    "        tokens = t.tensor(tokens).unsqueeze(0).to(self.device)\n",
    "        return self.get_logits(tokens)\n",
    "\n",
    "    def get_last_activations(self, layer):\n",
    "        return self.model.model.layers[layer].activations\n",
    "\n",
    "    def set_add_activations(self, layer, activations):\n",
    "        self.model.model.layers[layer].add(activations)\n",
    "\n",
    "    def set_calc_dot_product_with(self, layer, vector):\n",
    "        self.model.model.layers[layer].calc_dot_product_with = vector\n",
    "\n",
    "    def get_dot_products(self, layer):\n",
    "        return self.model.model.layers[layer].dot_products\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.reset()\n",
    "\n",
    "    def print_decoded_activations(self, decoded_activations, label, topk=10):\n",
    "        data = self.get_activation_data(decoded_activations, topk)[0]\n",
    "        print(label, data)\n",
    "\n",
    "    def decode_all_layers(\n",
    "        self,\n",
    "        tokens,\n",
    "        topk=10,\n",
    "        print_attn_mech=True,\n",
    "        print_intermediate_res=True,\n",
    "        print_mlp=True,\n",
    "        print_block=True,\n",
    "    ):\n",
    "        tokens = tokens.to(self.device)\n",
    "        self.get_logits(tokens)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            print(f\"Layer {i}: Decoded intermediate outputs\")\n",
    "            if print_attn_mech:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.attn_out_unembedded, \"Attention mechanism\", topk=topk\n",
    "                )\n",
    "            if print_intermediate_res:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.intermediate_resid_unembedded,\n",
    "                    \"Intermediate residual stream\",\n",
    "                    topk=topk,\n",
    "                )\n",
    "            if print_mlp:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.mlp_out_unembedded, \"MLP output\", topk=topk\n",
    "                )\n",
    "            if print_block:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.block_output_unembedded, \"Block output\", topk=topk\n",
    "                )\n",
    "\n",
    "    def plot_decoded_activations_for_layer(self, layer_number, tokens, topk=10):\n",
    "        tokens = tokens.to(self.device)\n",
    "        self.get_logits(tokens)\n",
    "        layer = self.model.model.layers[layer_number]\n",
    "\n",
    "        data = {}\n",
    "        data[\"Attention mechanism\"] = self.get_activation_data(\n",
    "            layer.attn_out_unembedded, topk\n",
    "        )[1]\n",
    "        data[\"Intermediate residual stream\"] = self.get_activation_data(\n",
    "            layer.intermediate_resid_unembedded, topk\n",
    "        )[1]\n",
    "        data[\"MLP output\"] = self.get_activation_data(layer.mlp_out_unembedded, topk)[1]\n",
    "        data[\"Block output\"] = self.get_activation_data(\n",
    "            layer.block_output_unembedded, topk\n",
    "        )[1]\n",
    "\n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 6))\n",
    "        fig.suptitle(f\"Layer {layer_number}: Decoded Intermediate Outputs\", fontsize=21)\n",
    "\n",
    "        for ax, (mechanism, values) in zip(axes.flatten(), data.items()):\n",
    "            tokens, scores = zip(*values)\n",
    "            ax.barh(tokens, scores, color=\"skyblue\")\n",
    "            ax.set_title(mechanism)\n",
    "            ax.set_xlabel(\"Value\")\n",
    "            ax.set_ylabel(\"Token\")\n",
    "\n",
    "            # Set scientific notation for x-axis labels when numbers are small\n",
    "            ax.xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "            ax.ticklabel_format(style=\"sci\", scilimits=(0, 0), axis=\"x\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def get_activation_data(self, decoded_activations, topk=10):\n",
    "        softmaxed = t.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n",
    "        values, indices = t.topk(softmaxed, topk)\n",
    "        probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n",
    "        return list(zip(tokens, probs_percent)), list(zip(tokens, values.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HF model\n",
    "## NOTE - Due to computation constraints, we loaded up these models separately. You can comment out one loader and use the other. Comment out relevant code in following blocks using that.\n",
    "import torch\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "cache_dir = \"./\"\n",
    "hf_model = HuggingfaceModel(model_name, cache_dir, max_new_tokens=200)\n",
    "\n",
    "model_dict_slt_ent = joblib.load('model_dict_slt_ent.pkl')\n",
    "model_dict_tbg_ent = joblib.load('model_dict_tbg_ent.pkl')\n",
    "\n",
    "layer = 13\n",
    "hall_vec = torch.load(f\"CAA/normalized_vectors/hallucination/vec_layer_{layer}_Llama-2-7b-chat-hf.pt\")\n",
    "ref_vec = torch.load(f\"CAA/normalized_vectors/refusal/vec_layer_{layer}_Llama-2-7b-chat-hf.pt\")\n",
    "\n",
    "\n",
    "# adjust the multipliers for 1xHallucination and 2xHallucination\n",
    "hall_multiplier = -2\n",
    "ref_multiplier = 2\n",
    "\n",
    "# load the model to steer (separate from other model)\n",
    "hf_model2 = LlamaWrapper(model_path = \"meta-llama/Llama-2-7b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## END to END flow doing generations and running probes\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "chunk_size = 100\n",
    "for start_index in range(0, len(combined_samples_test), chunk_size):\n",
    "    end_index = min(start_index + chunk_size, len(combined_samples_train))\n",
    "    chunk = combined_samples_train.iloc[start_index:end_index]\n",
    "\n",
    "    # Initialize lists for this chunk\n",
    "    combined_rows = []\n",
    "\n",
    "    for index, row in chunk.iterrows():\n",
    "        print(index)\n",
    "        real_answer = row['answers']['text'] if row['answers']['text'] else ''\n",
    "        if real_answer=='' or row['answers']['text']==[]:\n",
    "            real_answer = [\"The answer is not found in the context, so this question cannot be answered.\"]\n",
    "        \n",
    "        for _ in range(1):\n",
    "            # # Generate model predictions for base prompts\n",
    "            base_output_text, base_hidden_states = hf_model.predict(row['base_prompt'], temperature=1.0, return_latent=True)\n",
    "            base_answer = base_output_text[len(row['base_prompt']):].strip()\n",
    "            model_probs_preds_base = {\"base_answer\": base_answer}\n",
    "\n",
    "            # model_probs_preds_base = {}\n",
    "            \n",
    "            print(f\"Non steered: {base_answer}\")\n",
    "\n",
    "            ## RUN PROBES\n",
    "            sec_last_token_embedding = base_hidden_states[1]\n",
    "            last_tok_bef_gen_embedding = base_hidden_states[-1]\n",
    "\n",
    "            for layer_num in range(sec_last_token_embedding.shape[0]):\n",
    "                # Process second last token\n",
    "                slt_high_ent_prob = model_dict_slt_ent[layer_num].predict_proba(\n",
    "                    [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "                )[0][1]\n",
    "                slt_high_ent_pred = model_dict_slt_ent[layer_num].predict(\n",
    "                    [np.asarray(sec_last_token_embedding[layer_num][0])]\n",
    "                )[0]\n",
    "                model_probs_preds_base[f\"base_slt_layer_{layer_num}_prob\"] = slt_high_ent_prob\n",
    "                # model_probs_preds_base[f\"base_slt_layer_{layer_num}_pred\"] = slt_high_ent_pred\n",
    "\n",
    "                # Process last token before generation\n",
    "                tbg_high_ent_prob = model_dict_tbg_ent[layer_num].predict_proba(\n",
    "                    [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "                )[0][1]\n",
    "                tbg_high_ent_pred = model_dict_tbg_ent[layer_num].predict(\n",
    "                    [np.asarray(last_tok_bef_gen_embedding[layer_num][0])]\n",
    "                )[0]\n",
    "                model_probs_preds_base[f\"base_tbg_layer_{layer_num}_prob\"] = tbg_high_ent_prob\n",
    "                # model_probs_preds_base[f\"base_tbg_layer_{layer_num}_pred\"] = tbg_high_ent_pred\n",
    "\n",
    "            ## hall\n",
    "            hf_model2.reset_all()\n",
    "            hf_model2.set_add_activations(layer, hall_multiplier*hall_vec.cuda(device=\"cuda:1\"))\n",
    "            hall_steer_ans = hf_model2.generate_text(row['base_prompt'], max_new_tokens=200)\n",
    "            hall_steer_ans = hall_steer_ans.split(\"[/INST]\")[-1].replace(\"</s>\", \"\").strip()\n",
    "            \n",
    "            \n",
    "            model_probs_preds_base[\"base_hall_steer_answer\"] = hall_steer_ans\n",
    "            \n",
    "\n",
    "            # ref\n",
    "            hf_model2.reset_all()\n",
    "            hf_model2.set_add_activations(layer, ref_multiplier*ref_vec.cuda(device=\"cuda:1\"))\n",
    "            ref_steer_ans = hf_model2.generate_text(row['base_prompt'], max_new_tokens=200)\n",
    "            ref_steer_ans = ref_steer_ans.split(\"[/INST]\")[-1].replace(\"</s>\", \"\").strip()\n",
    "            \n",
    "\n",
    "            model_probs_preds_base[\"base_ref_steer_answer\"] = ref_steer_ans\n",
    "            \n",
    "\n",
    "            # Combine the current row with its predictions into a single dictionary\n",
    "            combined_row = row.to_dict()  # Convert the original row to a dictionary\n",
    "            combined_row.update(model_probs_preds_base)  # Add base predictions\n",
    "    \n",
    "\n",
    "            # Append the combined row to the list\n",
    "            combined_rows.append(combined_row)\n",
    "\n",
    "    # Create a DataFrame from the combined rows\n",
    "    chunk_with_predictions = pd.DataFrame(combined_rows)\n",
    "\n",
    "    # Save the chunk with an identifier\n",
    "    chunk_identifier = f\"{start_index}_{end_index - 1}\"\n",
    "    filename = f\"test_twox_latest_B_prompt_processed_chunk_{chunk_identifier}.csv\"\n",
    "    chunk_with_predictions.to_csv(filename, index=False)\n",
    "    print(f\"Saved chunk: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
