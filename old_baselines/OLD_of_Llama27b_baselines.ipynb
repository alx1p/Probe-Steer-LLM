{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjE9arAh38ra"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Dilan Nana, Walid Rahman, Alex Popa\"\n",
        "__version__ = \"CS230 Final Project, Fall Quarter 2024\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1GpXFEXzzB2",
        "outputId": "f0e6f3d3-9f1e-421d-a7ed-02c70fd0c5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            83Gi       1.0Gi        78Gi       1.0Mi       4.5Gi        81Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4BaVUjy0UgR",
        "outputId": "8447b5a6-e7d1-4e6b-eaa3-c5288d9fd528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 18 03:40:27 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnnbzT3H0ZJ6",
        "outputId": "66458141-bec0-43e8-d288-4fbc54204f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbyXE8KVpLEV"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1yRuGTn8qfg",
        "outputId": "0c29a29a-33f5-482e-b8be-c4be37b1edfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT7cV2PDxLKe"
      },
      "outputs": [],
      "source": [
        "main_directory = \"/content/drive/MyDrive/CS230\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnHtMuBHt_Dc"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XhNGA1J2lJMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9532841e-da9c-480b-e38b-70d5ec525799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets evaluate torch matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCHFhUV15Zc0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import torch\n",
        "import pickle\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "65tLqBQS8QN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c532d298-891f-4c9d-b76b-422cbfa7030b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 18 03:41:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Confirm correct runtime type\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1ZKy2juJnMb"
      },
      "source": [
        "# Semantic Coherence Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_5k6bM-Jte4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759c9685-d66f-42b8-f56b-c2df72d4edc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8M1UYcsKSf6"
      },
      "source": [
        "### Semantic Textual Similarity (STS) Eval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "711f7cc101f34ec2a8b25a5403eb6ed7",
            "2681e6b5672f4504860a18ce09c6c0de",
            "947547694f304cc7a3edd9157043cfcc",
            "f455cbd2833a4ec2997f47ee4aa20632",
            "e2f3be9888164d0e8f5bfa9fcc3b1c66",
            "152381714d41414d892e82540e6d3010",
            "40dd58160a194fa4a8128be7db3a6ec3",
            "6c76eab24e1a4c78a9b4f41389c7c231",
            "9b9ba6e59de14d2d90c388003a4e4e26",
            "7d87dfdac1a54904a0171f2c8349337b",
            "1d5131e06cca4ef2bf0a7c39189d9d32"
          ]
        },
        "id": "l2vcZJsHXPGs",
        "outputId": "5ee7df64-6b63-44eb-ece9-27facfb5d2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "711f7cc101f34ec2a8b25a5403eb6ed7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhNPFv2DJy5Y"
      },
      "outputs": [],
      "source": [
        "def compute_sts(predictions, references, model_name='all-MiniLM-L6-v2'):\n",
        "  \"\"\"\n",
        "    Computes Semantic Textual Similarity (STS) between predictions and references.\n",
        "\n",
        "    Args:\n",
        "        predictions (list of string): Generated answers.\n",
        "        references (list of string): Ground truth answers.\n",
        "        model_name (string): Pre-trained model name from sentence-transformers.\n",
        "\n",
        "    Returns:\n",
        "        float: Average cosine similarity score.\n",
        "    \"\"\"\n",
        "  model = SentenceTransformer(model_name)\n",
        "  embeddings1 = model.encode(predictions, convert_to_tensor=True)\n",
        "  embeddings2 = model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "  cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "  # Extract diagonal scores (prediction[i] vs reference[i])\n",
        "  diagonal_scores = cosine_scores.diagonal()\n",
        "  average_score = diagonal_scores.mean().item()\n",
        "\n",
        "  return average_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKQz_dZKmJm"
      },
      "source": [
        "### BERT Score Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0XBpHFmKsaE"
      },
      "outputs": [],
      "source": [
        "def compute_bertscore(predictions, references, lang='en'):\n",
        "    \"\"\"\n",
        "    Computes BERTScore for a list of predictions and references.\n",
        "\n",
        "    Args:\n",
        "        predictions (list of str): Generated answers.\n",
        "        references (list of str): Ground truth answers.\n",
        "        lang (str): Language of the texts.\n",
        "\n",
        "    Returns:\n",
        "        dict: BERTScore metrics (precision, recall, F1).\n",
        "    \"\"\"\n",
        "    P, R, F1 = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
        "    return {\n",
        "        'BERTScore_Precision': P.mean().item(),\n",
        "        'BERTScore_Recall': R.mean().item(),\n",
        "        'BERTScore_F1': F1.mean().item()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-qtB3uMKyeu"
      },
      "source": [
        "### Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7RfT4vSK210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e890d4c733884403b728e5c077ddc4d6",
            "7ba61c2ed5444ab988c2533e73dbdd27",
            "71e272a121d84475b06ed8258ac32e62",
            "b8c55e990c824d6cb5ec69f07309f639",
            "4e9853e906f940cbbde8c100eb96ac8e",
            "703c06f835a2431d87b0ea2999eb1211",
            "1b332143fba54714b36c92b419ab60ee",
            "1b621f250eaf4d2ba262ca696931a40e",
            "d5256cfa3f714eeaadb8cd880972b8b5",
            "7acfdc98a68b48d38ca4f97de1dce367",
            "10fc5503fc134022a2474aab1ca023a3",
            "5441aee34eec40ea81d07d9eb79cb7bb",
            "5b70300cff0b4bfb9c5f33a2a0702fa1",
            "4d5ac47a8d31497b9c97fb7d94b7d144",
            "9d58ce27c0e54a1fb07d704bf886c38e",
            "00317d055ca64965a8a1351a0a24549b",
            "98541c353c1f496ea09f783537d41f5a",
            "d01f116d2c504bf09745acfc8ec83722",
            "0de531bc98b94b7cb59c82cd9d70071c",
            "1b73f62fca0e4a81bab30d5851688a55",
            "cc86a221dc574769bc01dea57960f787",
            "abf55080cd5e4c88b45bf69fca76f2d0"
          ]
        },
        "outputId": "2cc4e94d-4a4f-441e-b04b-5703f81cb779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e890d4c733884403b728e5c077ddc4d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5441aee34eec40ea81d07d9eb79cb7bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 0.05 seconds, 57.61 sentences/sec\n",
            "BERTScore Results: {'BERTScore_Precision': 0.936948835849762, 'BERTScore_Recall': 0.9356613159179688, 'BERTScore_F1': 0.9362857937812805}\n",
            "Semantic Textual Similarity (STS) Score: 0.8982057571411133\n"
          ]
        }
      ],
      "source": [
        "# DEMO\n",
        "predictions = [\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"The theory of relativity was developed by Albert Einstein.\",\n",
        "    \"The fury of the gods is a fantasy Nordic novel in a series by J.Gwynne\"\n",
        "]\n",
        "\n",
        "references = [\n",
        "    \"Paris is France's capital city.\",\n",
        "    \"Albert Einstein developed the theory of relativity.\",\n",
        "    \"John Gwynne wrote a Nordic fantasy novel series, with one book being the Shadow of the Gods\"\n",
        "]\n",
        "\n",
        "# Compute BERTScore\n",
        "bertscore_results = compute_bertscore(predictions, references, lang='en')\n",
        "print(\"BERTScore Results:\", bertscore_results)\n",
        "\n",
        "# Compute STS\n",
        "sts_score = compute_sts(predictions, references)\n",
        "print(\"Semantic Textual Similarity (STS) Score:\", sts_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHwlT07GLDaU"
      },
      "source": [
        "## Complete Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Y0IVMALIBT"
      },
      "outputs": [],
      "source": [
        "def evaluate_answers(predictions, references):\n",
        "    \"\"\"\n",
        "    Evaluates the generated answers against the reference answers using ROUGE, BLEU, BERTScore, and STS.\n",
        "\n",
        "    Args:\n",
        "        predictions (list of str): The generated answers.\n",
        "        references (list of str): The reference (ground truth) answers.\n",
        "\n",
        "    Returns:\n",
        "        dict: The evaluation metrics.\n",
        "    \"\"\"\n",
        "    # Load evaluation metrics\n",
        "    rouge = evaluate.load('rouge')\n",
        "    bleu = evaluate.load('bleu')\n",
        "    bertscore = evaluate.load('bertscore')\n",
        "    print(predictions)\n",
        "    print(references)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    bleu_scores = bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "    # Compute BERTScore\n",
        "    bert_scores = bertscore.compute(predictions, references=references, lang='en', model_type=\"distilbert-base-uncased\", verbose=False)\n",
        "\n",
        "    #ref from HuggingFace: bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
        "\n",
        "    # bertscore = {\n",
        "    #     'BERTScore_Precision': P.mean().item(),\n",
        "    #     'BERTScore_Recall': R.mean().item(),\n",
        "    #     'BERTScore_F1': F1.mean().item()\n",
        "    # }\n",
        "\n",
        "    # # Compute STS\n",
        "    # sts = compute_sts(predictions, combined_references)\n",
        "\n",
        "    # # print(rouge_scores)\n",
        "    # # print(bleu_scores)\n",
        "    # # Correctness analysis\n",
        "    # correct_count = 0\n",
        "    # incorrect_count = 0\n",
        "    # for prediction, ref_list in zip(predictions, references):\n",
        "    #     if any(prediction.lower() == ref.lower() for ref in ref_list):\n",
        "    #         correct_count += 1\n",
        "    #     else:\n",
        "    #         incorrect_count += 1\n",
        "\n",
        "    # # Print Results\n",
        "    # print(f\"Total Predictions: {len(predictions)}\")\n",
        "    # print(f\"Correct Predictions: {correct_count}\")\n",
        "    # print(f\"Incorrect Predictions: {incorrect_count}\")\n",
        "    sts = 0\n",
        "\n",
        "    scores = {}\n",
        "    scores.update(rouge_scores)\n",
        "    scores.update(bleu_scores)\n",
        "    scores.update(bert_scores)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6HUY4yQVO4N"
      },
      "source": [
        "## Interpret Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ZL3qRfVR7P"
      },
      "outputs": [],
      "source": [
        "def interpret_metrics(metrics):\n",
        "    \"\"\"\n",
        "    Interprets evaluation metrics by categorizing their scores.\n",
        "\n",
        "    Args:\n",
        "        metrics (dict): A dictionary containing metric names and their scores.\n",
        "                        Example:\n",
        "                        {\n",
        "                            'rouge1': 0.7159,\n",
        "                            'rouge2': 0.3002,\n",
        "                            'rougeL': 0.4141,\n",
        "                            'bertscore_f1': 0.8250,\n",
        "                            'sts': 0.7500\n",
        "                        }\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with metrics, their scores, and dynamic descriptions.\n",
        "              Example:\n",
        "              {\n",
        "                  'rouge1': {'score': 0.7159, 'description': 'High content coverage.'},\n",
        "                  ...\n",
        "              }\n",
        "    \"\"\"\n",
        "    interpreted = {}\n",
        "\n",
        "    # Define thresholds for each metric\n",
        "    thresholds = {\n",
        "        'rouge1': {\n",
        "            'Low': 0.50,\n",
        "            'Moderate': 0.70\n",
        "        },\n",
        "        'rouge2': {\n",
        "            'Low': 0.20,\n",
        "            'Moderate': 0.40\n",
        "        },\n",
        "        'rougeL': {\n",
        "            'Low': 0.30,\n",
        "            'Moderate': 0.60\n",
        "        },\n",
        "        'bertscore_f1': {\n",
        "            'Low': 0.70,\n",
        "            'Moderate': 0.85\n",
        "        },\n",
        "        'sts': {\n",
        "            'Low': 0.60,\n",
        "            'Moderate': 0.80\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define descriptions based on categories\n",
        "    descriptions = {\n",
        "        'Low': {\n",
        "            'rouge1': 'Low content coverage.',\n",
        "            'rouge2': 'Low phrase overlap.',\n",
        "            'rougeL': 'Poor structural similarity.',\n",
        "            'bertscore_f1': 'Weak semantic alignment.',\n",
        "            'sts': 'Poor semantic coherence.'\n",
        "        },\n",
        "        'Moderate': {\n",
        "            'rouge1': 'Moderate content coverage.',\n",
        "            'rouge2': 'Moderate phrase overlap.',\n",
        "            'rougeL': 'Good structural similarity.',\n",
        "            'bertscore_f1': 'Good semantic alignment.',\n",
        "            'sts': 'Good semantic coherence.'\n",
        "        },\n",
        "        'High': {\n",
        "            'rouge1': 'High content coverage.',\n",
        "            'rouge2': 'Strong phrase overlap.',\n",
        "            'rougeL': 'Excellent structural similarity.',\n",
        "            'bertscore_f1': 'Strong semantic alignment.',\n",
        "            'sts': 'Excellent semantic coherence.'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for metric, score in metrics.items():\n",
        "        metric_lower = metric.lower()\n",
        "        if metric_lower in thresholds:\n",
        "            if score < thresholds[metric_lower]['Low']:\n",
        "                category = 'Low'\n",
        "            elif score < thresholds[metric_lower]['Moderate']:\n",
        "                category = 'Moderate'\n",
        "            else:\n",
        "                category = 'High'\n",
        "\n",
        "            description = descriptions[category][metric_lower]\n",
        "            interpreted[metric_lower] = {\n",
        "                'score': round(score, 4),\n",
        "                'description': description\n",
        "            }\n",
        "        else:\n",
        "            interpreted[metric_lower] = {\n",
        "                'score': round(score, 4),\n",
        "                'description': 'No interpretation available.'\n",
        "            }\n",
        "\n",
        "    return interpreted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_F9tIz_V6vQ"
      },
      "outputs": [],
      "source": [
        "#Dynamic option\n",
        "def interpret_metrics_dynamic(metrics, percentile_low=33, percentile_moderate=66):\n",
        "    \"\"\"\n",
        "    Interprets evaluation metrics by categorizing their scores based on dynamic percentile thresholds.\n",
        "\n",
        "    Args:\n",
        "        metrics (dict): A dictionary containing metric names and their scores.\n",
        "                        Example:\n",
        "                        {\n",
        "                            'rouge1': 0.7159,\n",
        "                            'rouge2': 0.3002,\n",
        "                            'rougeL': 0.4141,\n",
        "                            'bertscore_f1': 0.8250,\n",
        "                            'sts': 0.7500\n",
        "                        }\n",
        "        percentile_low (int): Lower percentile for 'Low' category.\n",
        "        percentile_moderate (int): Middle percentile for 'Moderate' category.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with metrics, their scores, and dynamic descriptions.\n",
        "    \"\"\"\n",
        "    interpreted = {}\n",
        "\n",
        "    # Collect scores for dynamic thresholding\n",
        "    all_scores = metrics.values()\n",
        "    low_threshold = np.percentile(list(all_scores), percentile_low)\n",
        "    moderate_threshold = np.percentile(list(all_scores), percentile_moderate)\n",
        "\n",
        "    for metric, score in metrics.items():\n",
        "        if score < low_threshold:\n",
        "            category = 'Low'\n",
        "        elif score < moderate_threshold:\n",
        "            category = 'Moderate'\n",
        "        else:\n",
        "            category = 'High'\n",
        "\n",
        "        # Define dynamic descriptions\n",
        "        if category == 'Low':\n",
        "            description = f\"{metric.upper()} is Low.\"\n",
        "        elif category == 'Moderate':\n",
        "            description = f\"{metric.upper()} is Moderate.\"\n",
        "        else:\n",
        "            description = f\"{metric.upper()} is High.\"\n",
        "\n",
        "        interpreted[metric] = {\n",
        "            'score': round(score, 4),\n",
        "            'description': description\n",
        "        }\n",
        "\n",
        "    return interpreted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeuIe1IiLYHO"
      },
      "source": [
        "## Save Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B46Iw46mLaqE"
      },
      "outputs": [],
      "source": [
        "def save_evaluation_results(results, save_dir, filename='llama2_7b_qa_baseline_semantic_coherence_metrics.csv'):\n",
        "    \"\"\"\n",
        "    Saves the evaluation metrics to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing evaluation metrics.\n",
        "        save_dir (str): Directory path to save the CSV file.\n",
        "        filename (str): Name of the CSV file.\n",
        "    \"\"\"\n",
        "    results_df = pd.DataFrame(results).transpose()\n",
        "    results_df = results_df.reset_index().rename(columns={'index': 'Dataset'})\n",
        "    csv_path = os.path.join(save_dir, filename)\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Evaluation results saved to '{csv_path}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnkmZfAnLuxR"
      },
      "source": [
        "## Plot Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yxBigXxLzam"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(results, metrics_to_plot=['rouge1', 'rouge2', 'rougeL', 'bleu', 'BERTScore_F1', 'STS']):\n",
        "    \"\"\"\n",
        "    Plots evaluation metrics for different datasets.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing evaluation metrics.\n",
        "        metrics_to_plot (list of str): List of metric names to plot.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results).transpose()\n",
        "    df = df.reset_index().rename(columns={'index': 'Dataset'})\n",
        "\n",
        "    # Melt the dataframe for easier plotting\n",
        "    df_melted = df.melt(id_vars='Dataset', value_vars=metrics_to_plot, var_name='Metric', value_name='Score')\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(x='Dataset', y='Score', hue='Metric', data=df_melted)\n",
        "    plt.title('Semantic Coherence Evaluation Metrics by Dataset')\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(title='Metric')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcXCewXwMAXf"
      },
      "source": [
        "# Beam Search Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QojIJWaLMgXu"
      },
      "source": [
        "## Creating beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xThLzF-NMLLr"
      },
      "outputs": [],
      "source": [
        "#Creating beam search for classic implementation\n",
        "class Beam:\n",
        "  def __init__(self, tokens, log_probs, is_finished=False):\n",
        "    self.tokens = tokens #token ID list\n",
        "    self.log_prob = log_probs #Cumulative log probability\n",
        "    self.is_finished = is_finished #gen <eos>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6VJhBWTMQiq"
      },
      "outputs": [],
      "source": [
        "def manual_beam_search(helper, prompt, max_length=100, num_beams=5, early_stopping=True, no_repeat_ngram_size=2):\n",
        "  \"\"\"\n",
        "  Performs manual Beam Search.\n",
        "\n",
        "  Args:\n",
        "      helper (Llama7BHelper): The helper class instance.\n",
        "      prompt (str): The input prompt.\n",
        "      max_length (int): Maximum length of the generated sequence.\n",
        "      num_beams (int): Number of beams.\n",
        "      early_stopping (bool): Whether to stop early when all beams have finished.\n",
        "      no_repeat_ngram_size (int): Size of n-grams that should not be repeated.\n",
        "\n",
        "  Returns:\n",
        "      str: The generated sequence with the highest score.\n",
        "  \"\"\"\n",
        "  tokenizer = helper.tokenizer\n",
        "  model = helper.model\n",
        "  device = helper.device\n",
        "  #Dilan come back to this\n",
        "  # Encode the prompt\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "  initial_beam = Beam(tokens=input_ids[0].tolist(), log_prob=0.0)\n",
        "  beams = [initial_beam]\n",
        "\n",
        "  for step in range(max_length):\n",
        "      all_candidates = []\n",
        "      for beam in beams:\n",
        "          if beam.is_finished:\n",
        "              # If the beam is already finished, carry it forward\n",
        "              all_candidates.append(beam)\n",
        "              continue\n",
        "\n",
        "          # Convert tokens to tensor\n",
        "          input_ids = torch.tensor([beam.tokens]).to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              outputs = model(input_ids)\n",
        "              logits = outputs.logits  # Shape: (1, seq_len, vocab_size)\n",
        "              next_token_logits = logits[0, -1, :]  # Shape: (vocab_size,)\n",
        "\n",
        "              # Apply softmax to get probabilities\n",
        "              probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "              log_probs = torch.log(probs)\n",
        "\n",
        "          # Get top 'num_beams' tokens\n",
        "          top_log_probs, top_indices = torch.topk(log_probs, num_beams)\n",
        "\n",
        "          for i in range(num_beams):\n",
        "              token_id = top_indices[i].item()\n",
        "              token_log_prob = top_log_probs[i].item()\n",
        "\n",
        "              new_tokens = beam.tokens + [token_id]\n",
        "              new_log_prob = beam.log_prob + token_log_prob\n",
        "\n",
        "              # Check for end-of-sequence token\n",
        "              if token_id == tokenizer.eos_token_id:\n",
        "                  is_finished = True\n",
        "              else:\n",
        "                  is_finished = False\n",
        "\n",
        "              # Check for no repeating n-grams\n",
        "              if no_repeat_ngram_size > 0:\n",
        "                  ngrams = zip(*[new_tokens[i:] for i in range(no_repeat_ngram_size)])\n",
        "                  ngram_set = set(ngrams)\n",
        "                  # No action needed here as we're preventing expansion that would create repeats\n",
        "\n",
        "              new_beam = Beam(tokens=new_tokens, log_prob=new_log_prob, is_finished=is_finished)\n",
        "              all_candidates.append(new_beam)\n",
        "\n",
        "      # Sort all candidates by log probability\n",
        "      ordered = sorted(all_candidates, key=lambda b: b.log_prob, reverse=True)\n",
        "\n",
        "      # Select top 'num_beams' beams\n",
        "      beams = ordered[:num_beams]\n",
        "\n",
        "      # If early stopping is enabled and all beams are finished, stop\n",
        "      if early_stopping and all(beam.is_finished for beam in beams):\n",
        "          break\n",
        "\n",
        "  # Select the beam with the highest log probability\n",
        "  best_beam = max(beams, key=lambda b: b.log_prob)\n",
        "  generated_sequence = tokenizer.decode(best_beam.tokens, skip_special_tokens=True)\n",
        "  return generated_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsuXu4T8M2iq"
      },
      "source": [
        "## Beam Search Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDXA3GLZNAwT"
      },
      "outputs": [],
      "source": [
        "def evaluate_llama_manual_beam_search(helper, dataset, max_length=100, num_beams=5):\n",
        "  predictions = []\n",
        "  references = []\n",
        "  for sample in tqdm(dataset, desc=\"Generating and Evaluating\"):\n",
        "    try:\n",
        "        if 'question' in sample and 'answers' in sample:\n",
        "            # SQuAD-like structure\n",
        "            question = sample['question']\n",
        "            reference = sample['answers']['text'][0]\n",
        "        elif 'question' in sample and 'answer' in sample:\n",
        "            # HotpotQA-like structure\n",
        "            question = sample['question']\n",
        "            reference = sample['answer']\n",
        "        elif 'question' in sample and 'answer' in sample:\n",
        "            # TriviaQA-like structure\n",
        "            question = sample['question']['query']\n",
        "            reference = sample['answer']['value'][0] if isinstance(sample['answer']['value'], list) else sample['answer']['value']\n",
        "        else:\n",
        "            raise ValueError(\"Unknown dataset structure.\")\n",
        "\n",
        "        # Generate answer using manual Beam Search\n",
        "        generated_answer = helper.generate_text_manual_beam_search(\n",
        "            prompt=question,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "        predictions.append(generated_answer)\n",
        "        references.append(reference)\n",
        "    except KeyError as ke:\n",
        "        print(f\"Missing key in sample: {ke}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample: {e}\")\n",
        "        continue\n",
        "\n",
        "  # Initialize SentenceTransformer model for STS\n",
        "  st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "  # Call our evaluation function for semantic coherence metrics\n",
        "  metrics = evaluate_answers(predictions, references, st_model)\n",
        "  return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvaVYbHhlnYH"
      },
      "outputs": [],
      "source": [
        "def save__beam_search_evaluation_results(results, save_dir, filename='llama2_7b_qa_beam_search_metrics.csv'):\n",
        "    \"\"\"\n",
        "    Saves the evaluation metrics to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing evaluation metrics.\n",
        "        save_dir (str): Directory path to save the CSV file.\n",
        "        filename (str): Name of the CSV file.\n",
        "    \"\"\"\n",
        "    results_df = pd.DataFrame(results).transpose()\n",
        "    results_df = results_df.reset_index().rename(columns={'index': 'Dataset'})\n",
        "    csv_path = os.path.join(save_dir, filename)\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Evaluation results saved to '{csv_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uktmgYb9lwTD"
      },
      "outputs": [],
      "source": [
        "def plot_beam_search_metrics(results, metrics_to_plot=['rouge1', 'rouge2', 'rougeL', 'bleu', 'BERTScore_F1', 'STS']):\n",
        "    \"\"\"\n",
        "    Plots evaluation metrics for different datasets.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing evaluation metrics.\n",
        "        metrics_to_plot (list of str): List of metric names to plot.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results).transpose()\n",
        "    df = df.reset_index().rename(columns={'index': 'Dataset'})\n",
        "\n",
        "    # Melt the dataframe for easier plotting\n",
        "    df_melted = df.melt(id_vars='Dataset', value_vars=metrics_to_plot, var_name='Metric', value_name='Score')\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(x='Dataset', y='Score', hue='Metric', data=df_melted)\n",
        "    plt.title('Semantic Coherence Evaluation Metrics by Dataset')\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(title='Metric')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwg73voLiZuK"
      },
      "source": [
        "# QA w/Standard Decoding Experiment (no-mods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMKGD68GJZ45"
      },
      "source": [
        "**WHAT PROMPT??**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhoO_TofNKTs"
      },
      "source": [
        "### Generate Text/Answers Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kig3qnwh7Jt"
      },
      "outputs": [],
      "source": [
        "#temp, repetition penalty etc.?\n",
        "#Llama team made these choices: https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/generation_config.json\n",
        "def generate_answers(model, tokenizer, questions, max_length=200, temperature=0.7, top_p=0.9, top_k=50, max_new_tokens=100, repetition_penalty=1.2, do_sample=True):\n",
        "    \"\"\"\n",
        "    Generates answers for a list of questions using the provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): The language model.\n",
        "        tokenizer (AutoTokenizer): The tokenizer.\n",
        "        questions (list of str): List of questions to answer.\n",
        "        max_length (int): Maximum length of the generated answer.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling probability.\n",
        "        top_k (int): Top-k sampling.\n",
        "        repetition_penalty (float): Penalty for repeating tokens.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Generated answers.\n",
        "    \"\"\"\n",
        "    generated_answers = []\n",
        "\n",
        "    for question in tqdm(questions, desc=\"Generating Answers\"):\n",
        "        prompt = f\"Question: {question}\\nAnswer:\"\n",
        "        # inputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "        print(prompt)\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True, #Pad so shorter inputs match longest batch\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "        # Generate response using standard decoding (greedy as default)\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens, #use max_length if inputs are consistently short\n",
        "            do_sample=do_sample,# False for greedy\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        # early_stopping=True, cant use with num_beams = 1\n",
        "\n",
        "        # outputs = model.generate(\n",
        "        #     input_ids,\n",
        "        #     max_length=max_length,\n",
        "        #     temperature=temperature,\n",
        "        #     top_p=top_p,\n",
        "        #     top_k=top_k,\n",
        "        #     repetition_penalty=repetition_penalty,\n",
        "        #     do_sample=do_sample,# False for greedy\n",
        "        #     eos_token_id=tokenizer.eos_token_id,\n",
        "        #     pad_token_id=tokenizer.eos_token_id\n",
        "        # )\n",
        "\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract the answer part after \"Answer:\"\n",
        "        answer = answer.split(\"Answer:\")[-1].strip()\n",
        "        generated_answers.append(answer)\n",
        "\n",
        "    return generated_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKjOH25Dm1a9"
      },
      "source": [
        "# Standard Activation Steering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yF5Bhgmm6Zj"
      },
      "source": [
        "## Forward hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5C6DonSm9FA"
      },
      "outputs": [],
      "source": [
        "class ActivationSteering:\n",
        "    def __init__(self, model, target_layer, mixing_activation, multiplier=10):\n",
        "      \"\"\"\n",
        "      Initializes the ActivationSteering instance.\n",
        "\n",
        "      Args:\n",
        "          model (AutoModelForCausalLM): The language model.\n",
        "          target_layer (int): The layer number where steering will be applied.\n",
        "          mixing_activation (torch.Tensor): The activation to mix.\n",
        "          multiplier (float): Scaling factor for the mixing activation.\n",
        "      \"\"\"\n",
        "      self.model = model\n",
        "      self.target_layer = target_layer\n",
        "      self.mixing_activation = mixing_activation.to(model.device)\n",
        "      self.multiplier = multiplier\n",
        "      self.hook = None\n",
        "      self.steered = False  # To ensure mixing is applied only once per generation\n",
        "\n",
        "    def get_target_layer_module(self):\n",
        "      \"\"\"\n",
        "      Retrieves the target layer's attention module.\n",
        "      \"\"\"\n",
        "      return self.model.model.layers[self.target_layer].self_attn\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "      \"\"\"\n",
        "      The hook function to modify attention values.\n",
        "\n",
        "      Args:\n",
        "          module: The module to which the hook is attached.\n",
        "          input: The input to the module.\n",
        "          output: The output from the module.\n",
        "      \"\"\"\n",
        "      if not self.steered:\n",
        "          # output[0] contains the attention weights\n",
        "          # output[1] contains the context (attention output)\n",
        "          # We'll modify the attention output\n",
        "          modified_context = output[1] + self.multiplier * self.mixing_activation\n",
        "          return (output[0], modified_context)\n",
        "      else:\n",
        "          return output\n",
        "\n",
        "    def apply_hook(self):\n",
        "      \"\"\"\n",
        "      Attaches the hook to the target layer's attention module.\n",
        "      \"\"\"\n",
        "      target_module = self.get_target_layer_module()\n",
        "      self.hook = target_module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def remove_hook(self):\n",
        "      \"\"\"\n",
        "      Removes the attached hook.\n",
        "      \"\"\"\n",
        "      if self.hook is not None:\n",
        "        self.hook.remove()\n",
        "        self.hook = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7uy3elnc7M"
      },
      "source": [
        "## Mixing Attention Preparation\n",
        "\n",
        "I need to figure out how to define the mixing input: A sentence or phrase that embodies the concept or persona you want to steer towards or away from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5h27CcKntfw"
      },
      "source": [
        "## Generating the Mixing Activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FawXMaon7mF"
      },
      "outputs": [],
      "source": [
        "def get_mixing_activation(model, tokenizer, mixing_input, target_layer, max_length=50):\n",
        "  \"\"\"\n",
        "  Generates the mixing activation from the mixing input.\n",
        "\n",
        "  Args:\n",
        "      model (AutoModelForCausalLM): The language model.\n",
        "      tokenizer (AutoTokenizer): The tokenizer.\n",
        "      mixing_input (str): The input to generate the mixing activation.\n",
        "      target_layer (int): The layer number from which to extract the activation.\n",
        "      max_length (int): Maximum length of the mixing input generation.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: The mixing activation to be added.\n",
        "  \"\"\"\n",
        "  # Encode the mixing input\n",
        "  prompt = f\"Statement: {mixing_input}\\nContinuation:\"\n",
        "  inputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "  # Disable gradients for mixing activation generation\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "          inputs,\n",
        "          max_length=max_length,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9,\n",
        "          top_k=50,\n",
        "          repetition_penalty=1.2,\n",
        "          do_sample=True,\n",
        "          eos_token_id=tokenizer.eos_token_id,\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "\n",
        "  # Decode the generated continuation\n",
        "  continuation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  continuation = continuation.split(\"Continuation:\")[-1].strip()\n",
        "\n",
        "  # Tokenize the mixing input and continuation\n",
        "  full_input = f\"Statement: {mixing_input}\\nContinuation: {continuation}\"\n",
        "  inputs = tokenizer.encode(full_input, return_tensors='pt').to(model.device)\n",
        "\n",
        "  # Register a forward hook to capture attention output at the target layer\n",
        "  mixing_activation = []\n",
        "\n",
        "  def hook_fn(module, input, output):\n",
        "      # output[0]: attention weights, output[1]: attention context\n",
        "      # We'll capture the attention context\n",
        "      mixing_activation.append(output[1].clone())\n",
        "\n",
        "  target_module = model.model.layers[target_layer].self_attn\n",
        "  hook = target_module.register_forward_hook(hook_fn)\n",
        "\n",
        "  # Forward pass to capture activation\n",
        "  with torch.no_grad():\n",
        "      model(inputs)\n",
        "\n",
        "  # Remove the hook\n",
        "  hook.remove()\n",
        "\n",
        "  if mixing_activation:\n",
        "      return mixing_activation[0]\n",
        "  else:\n",
        "      raise ValueError(\"Failed to capture mixing activation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htEwPShzN00j"
      },
      "source": [
        "# Main Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoD0m-L1N4jE"
      },
      "source": [
        "## Gather Model + Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrJKWL7x1cD4"
      },
      "source": [
        "### Llama Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMfRtPQcOMzL"
      },
      "outputs": [],
      "source": [
        "# Define your helper class\n",
        "class Llama7BHelper:\n",
        "  def __init__(self, pretrained_model=\"huggyllama/llama-7b\"):\n",
        "      self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "      self.model = AutoModelForCausalLM.from_pretrained(pretrained_model).to(self.device)\n",
        "      #explicitly set pad token b/c ran into issues, but Nina didn't?\n",
        "      if self.tokenizer.pad_token_id is None:\n",
        "        self.tokenizer.pad_token_id = self.tokenizer.pad_token_id = 0 #got this from generation_config.json\n",
        "        self.tokenizer.pad_token=\"[PAD]\"\n",
        "\n",
        "  #taken from nrimsky github\n",
        "  def get_logits(self, prompt):\n",
        "    inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "    with torch.no_grad():\n",
        "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "    return logits\n",
        "\n",
        "  def generate_text_manual_beam_search(self, prompt, max_length=100, num_beams=5, early_stopping=True, no_repeat_ngram_size=2):\n",
        "        return manual_beam_search(\n",
        "            helper=self,\n",
        "            prompt=prompt,\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=early_stopping,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aE6qIflw1JBR"
      },
      "outputs": [],
      "source": [
        "# #Specify model\n",
        "# pretrained_model=\"huggyllama/llama-7b\"\n",
        "\n",
        "# #Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "# #Load model\n",
        "# model = AutoModelForCausalLM.from_pretrained(pretrained_model, device_map=\"auto\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "\n",
        "# # Initialize the generation pipeline\n",
        "# qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IitvGfrl1flG"
      },
      "source": [
        "### Load or Download Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2Y_391kIg1V"
      },
      "outputs": [],
      "source": [
        "#Load model from drive if exists\n",
        "def load_or_download_model(model_load_path):\n",
        "  if os.path.exists(model_load_path) and os.listdir(model_load_path):\n",
        "    print(\"Loading model from Google Drive...\")\n",
        "    helper = Llama7BHelper(pretrained_model=model_load_path)\n",
        "  else:\n",
        "    print(\"We could not find model in Google Drive. Downloading from Hugging Face...\")\n",
        "    helper = Llama7BHelper(pretrained_model=\"huggyllama/llama-7b\")\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(model_load_path, exist_ok=True)\n",
        "    print(\"Saving model to Google Drive...\")\n",
        "    helper.tokenizer.save_pretrained(model_load_path)\n",
        "    helper.model.save_pretrained(model_load_path)\n",
        "    print(f\"Ok! We've saved model and tokenizer to {model_load_path}\")\n",
        "\n",
        "  return helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bttOnlO21jd"
      },
      "outputs": [],
      "source": [
        "# #Save model to drive folder for future use\n",
        "# model_save_dir = \"/content/drive/MyDrive/CS230/Llama2-7b-Model\"\n",
        "# # Define the path to save the model\n",
        "\n",
        "\n",
        "# # Save the tokenizer and model\n",
        "# tokenizer.save_pretrained(model_save_dir)\n",
        "# model.save_pretrained(model_save_dir)\n",
        "\n",
        "# print(f\"Model and tokenizer saved to {model_save_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJsEm9W18dc6"
      },
      "source": [
        "## Loading/Processing Datasets Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IBwFVsUstCq"
      },
      "outputs": [],
      "source": [
        "#Save datasets to drive if first time using\n",
        "def save_datasets_as_pickle(datasets, save_dir):\n",
        "  for name, dataset in datasets.items():\n",
        "      df = dataset.to_pandas()\n",
        "      file_path = f\"{save_dir}/{name}.pkl\"\n",
        "      df.to_pickle(file_path)\n",
        "      print(f\"Saved {name} to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "recmKYJ0vB8y"
      },
      "outputs": [],
      "source": [
        "#Get small sample per dataset\n",
        "def get_shuffled_small_dataset(datasets, sample_size=100, seed=42):\n",
        "  sampled_datasets={}\n",
        "  for name, dataset in datasets.items():\n",
        "    sampled_dataset = dataset.shuffle(seed=seed).select(range(sample_size))\n",
        "    sampled_datasets[name] = sampled_dataset\n",
        "    print(f\"{name} sampled with {len(sampled_dataset)} examples.\")\n",
        "  return sampled_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KEPmDtdz8g3S"
      },
      "outputs": [],
      "source": [
        "def download_baseline_datasets(save_dir, datasets={}):\n",
        "  # Load SQuAD, HotpotQA, and TriviaQA datasets\n",
        "  # SQuAD v2.0\n",
        "  datasets['squad'] = load_dataset('squad_v2', split='validation')\n",
        "\n",
        "  # Load HotpotQA\n",
        "  datasets['hotpotqa'] = load_dataset('hotpot_qa', 'distractor', split='validation')\n",
        "\n",
        "  # Load TriviaQA (web version)\n",
        "  #https://docs.allennlp.org/models/main/models/rc/dataset_readers/triviaqa/ <-- helpful for future use\n",
        "  datasets['triviaqa'] = load_dataset('trivia_qa', 'unfiltered', split='validation')\n",
        "\n",
        "  #Save dataset portions to drive folder for fast reloading\n",
        "  save_datasets_as_pickle(datasets, save_dir)\n",
        "\n",
        "  # Display the number of samples in each dataset\n",
        "  for name, dataset in datasets.items():\n",
        "      print(f\"{name.capitalize()} has {len(dataset)} samples.\")\n",
        "\n",
        "  return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Nw3_-sw6w0"
      },
      "outputs": [],
      "source": [
        "# for filename in os.listdir(main_directory):\n",
        "#   loaded_datasets = {}\n",
        "#   if filename.endswith(\".pkl\"):\n",
        "#     filepath = os.path.join(main_directory, filename)\n",
        "#     dataset_name = filename.replace(\".pkl\", \"\")  # Remove the .pkl extension for the name\n",
        "\n",
        "#     # Load the pickle file\n",
        "#     with open(filepath, 'rb') as f:\n",
        "#       pandas_df = pickle.load(f)\n",
        "\n",
        "#     # Convert to Hugging Face Dataset\n",
        "#     loaded_datasets[dataset_name] = Dataset.from_pandas(pandas_df)\n",
        "#     print(f\"Loaded and converted {filename} to Hugging Face Dataset.\")\n",
        "\n",
        "#   print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftQxBj5owqBS"
      },
      "outputs": [],
      "source": [
        "#Load datasets or retrieve\n",
        "def load_datasets_from_pickle(save_dir):\n",
        "    \"\"\"\n",
        "    Loads each dataset from its corresponding Pickle file.\n",
        "\n",
        "    Args:\n",
        "        save_dir (str): Directory path in Google Drive where files are saved.\n",
        "        sample_size (int): Number of samples expected in each Pickle file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing loaded datasets.\n",
        "    \"\"\"\n",
        "    loaded_datasets = {}\n",
        "    for filename in os.listdir(save_dir):\n",
        "      if filename.endswith(\".pkl\"):\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        dataset_name = filename.replace(\".pkl\", \"\")  # Remove the .pkl extension for the name\n",
        "\n",
        "        # Load the pickle file\n",
        "        with open(filepath, 'rb') as f:\n",
        "          pandas_df = pickle.load(f)\n",
        "\n",
        "        # Convert to Hugging Face Dataset\n",
        "        loaded_datasets[dataset_name] = Dataset.from_pandas(pandas_df)\n",
        "        print(f\"Loaded and converted {filename} to data frame.\")\n",
        "      print(filename)\n",
        "    return loaded_datasets\n",
        "\n",
        "    # for name in dataset_names:\n",
        "    #     file_path = os.path.join(save_dir, f\"{name}.pkl\")\n",
        "    #     if os.path.exists(file_path):\n",
        "    #         df = pd.read_pickle(file_path)\n",
        "    #         # Convert DataFrame back to Hugging Face Dataset\n",
        "    #         dataset = load_dataset('csv', data_files=file_path, split='train')\n",
        "    #         loaded_datasets[name] = dataset\n",
        "    #         print(f\"Loaded {name} from {file_path}\")\n",
        "    #     else:\n",
        "    #         print(f\"{name}.pkl does not exist in {save_dir}.\")\n",
        "    #         loaded_datasets = download_baseline_datasets(save_dir)\n",
        "    # return loaded_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nf5vIBVTZpX"
      },
      "source": [
        "## Display Sample Data Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zle1df0vwFcE"
      },
      "outputs": [],
      "source": [
        "def display_examples_per_dataset(smaller_datasets, sample_size=5):\n",
        "  \"\"\"\n",
        "  Displays subset from SQuAD, HotpotQA, and TriviaQA datasets.\n",
        "\n",
        "  \"\"\"\n",
        "  for name, dataset in datasets.items():\n",
        "    small_sample = dataset.select(range(sample_size))\n",
        "    if name == 'squad':\n",
        "      print(\"=== SQuAD Samples ===\")\n",
        "      for idx, sample in enumerate(small_sample):\n",
        "          print(f\"\\nSample {idx+1}:\")\n",
        "          print(f\"Question: {sample['question']}\")\n",
        "          print(f\"Answer: {sample['answers']['text'][0]}\")\n",
        "    elif name == 'hotpotqa':\n",
        "       print(\"\\n=== HotpotQA Samples ===\")\n",
        "       for idx, sample in enumerate(small_sample):\n",
        "          print(f\"\\nSample {idx+1}:\")\n",
        "          print(f\"Question: {sample['question']}\")\n",
        "          print(f\"Answer: {sample['answer']}\")\n",
        "    else:\n",
        "      print(\"\\n=== TriviaQA Samples ===\")\n",
        "      for idx, sample in enumerate(small_sample):\n",
        "          print(f\"\\nSample {idx+1}:\")\n",
        "          print(f\"Question: {sample['question']}\")\n",
        "          answer = sample['answer']['value']\n",
        "          # Handle list or single answer\n",
        "          if isinstance(answer, list):\n",
        "              answer = answer[0]\n",
        "          print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H17YU2NbWJKJ"
      },
      "source": [
        "## Running commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjpDB1k1I_Z"
      },
      "source": [
        "### Get Data From Drive + Shuffle/Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKFhiMQgzALI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13b43b8-6b49-4588-eb60-744d9b23d47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res.png\n",
            "Loaded and converted squad.pkl to data frame.\n",
            "squad.pkl\n",
            "Loaded and converted hotpotqa.pkl to data frame.\n",
            "hotpotqa.pkl\n",
            "Loaded and converted triviaqa.pkl to data frame.\n",
            "triviaqa.pkl\n",
            "Llama2-7b-Model\n",
            "Untitled0.ipynb\n",
            "230 Project Idea.gdoc\n",
            "Llama27b_baselines.ipynb\n",
            "squad sampled with 100 examples.\n",
            "hotpotqa sampled with 100 examples.\n",
            "triviaqa sampled with 100 examples.\n",
            "=== SQuAD Samples ===\n",
            "\n",
            "Sample 1:\n",
            "Question: In what country is Normandy located?\n",
            "Answer: France\n",
            "\n",
            "Sample 2:\n",
            "Question: When were the Normans in Normandy?\n",
            "Answer: 10th and 11th centuries\n",
            "\n",
            "Sample 3:\n",
            "Question: From which countries did the Norse originate?\n",
            "Answer: Denmark, Iceland and Norway\n",
            "\n",
            "Sample 4:\n",
            "Question: Who was the Norse leader?\n",
            "Answer: Rollo\n",
            "\n",
            "Sample 5:\n",
            "Question: What century did the Normans first gain their separate identity?\n",
            "Answer: 10th century\n",
            "\n",
            "=== HotpotQA Samples ===\n",
            "\n",
            "Sample 1:\n",
            "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "Answer: yes\n",
            "\n",
            "Sample 2:\n",
            "Question: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\n",
            "Answer: Chief of Protocol\n",
            "\n",
            "Sample 3:\n",
            "Question: What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\n",
            "Answer: Animorphs\n",
            "\n",
            "Sample 4:\n",
            "Question: Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?\n",
            "Answer: no\n",
            "\n",
            "Sample 5:\n",
            "Question: The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?\n",
            "Answer: Greenwich Village, New York City\n",
            "\n",
            "=== TriviaQA Samples ===\n",
            "\n",
            "Sample 1:\n",
            "Question: Who was the man behind The Chipmunks?\n",
            "Answer: David Seville\n",
            "\n",
            "Sample 2:\n",
            "Question: What star sign is Jamie Lee Curtis?\n",
            "Answer: Scorpio\n",
            "\n",
            "Sample 3:\n",
            "Question: Which Lloyd Webber musical premiered in the US on 10th December 1993?\n",
            "Answer: Sunset Boulevard\n",
            "\n",
            "Sample 4:\n",
            "Question: Who was the next British Prime Minister after Arthur Balfour?\n",
            "Answer: Campbell-Bannerman\n",
            "\n",
            "Sample 5:\n",
            "Question: Who had a 70s No 1 hit with Kiss You All Over?\n",
            "Answer: Exile\n"
          ]
        }
      ],
      "source": [
        "#Retrieve data\n",
        "datasets = {}\n",
        "if not datasets:\n",
        "  datasets = load_datasets_from_pickle(\"/content/drive/MyDrive/CS230\")\n",
        "else:\n",
        "  print(\"We're all good! Datasets recovered\")\n",
        "  print(\"actually need to download from helper function\")\n",
        "sample_size = 100  # Adjust as needed\n",
        "smaller_datasets = get_shuffled_small_dataset(datasets, sample_size=sample_size, seed=42)\n",
        "display_examples_per_dataset(smaller_datasets, sample_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MafbiDRzEIc"
      },
      "outputs": [],
      "source": [
        "# references = [\n",
        "#     sample[\"text\"][0] if sample[\"text\"] else \"\"  # Use the first answer if available, otherwise empty string\n",
        "#     for sample in smaller_datasets[\"squad\"][\"answers\"]\n",
        "# ]\n",
        "# print(references)\n",
        "\n",
        "'''\n",
        "question: xxxx\n",
        "valid_answers: ['x1', 'x2', 'x3']\n",
        "prediction: 'blue'\n",
        "\n",
        "3 predictions\n",
        "blue -> x1\n",
        "blue -> x2\n",
        "blue -> x3\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_datasets['squad'][0]['answers']['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxWnPKv50f8T",
        "outputId": "c2eb14f0-7887-4023-c39c-fb40e48cd799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fort Presque Isle (near present-day Erie, Pennsylvania',\n",
              " 'Fort Presque Isle',\n",
              " 'near present-day Erie, Pennsylvania',\n",
              " 'Fort Presque Isle',\n",
              " 'near present-day Erie, Pennsylvania']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edom27061SfP"
      },
      "source": [
        "### Get Model From Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA4DXCcmTL9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5ac27e9fccce46a0bfa0ac148af4bf06",
            "051ed2071efe4d658fddc272ea0255ba",
            "dc9e487b9627407b97f63facd241547d",
            "a463d0eb4faa4d3f99ba9558be4298c3",
            "f9360d53ae0343ea85a1866dc91815d5",
            "43c0842408a344759d73f1952ab73841",
            "bc931b0521de4b1c9496465043d31537",
            "6c19da03a6f3401697b7a56b4dd6251b",
            "4cfd90b935ca47069bc0ff51e12a0d8b",
            "b1eceb7e709e41489859cd25c45d6f07",
            "9587bb20d6754ed2b3618e3d580ea46a"
          ]
        },
        "outputId": "e36ac8a9-ab46-43df-9b38-b3c324fe744b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from Google Drive...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac27e9fccce46a0bfa0ac148af4bf06"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Retreive model\n",
        "model_load_path = \"/content/drive/MyDrive/CS230/Llama2-7b-Model\"\n",
        "helper = load_or_download_model(model_load_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWbfc7Lhi5DZ"
      },
      "source": [
        "### Running Standard Decoding Experiment Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_basic_experiment(wrapped_model=helper, sampled_datasets={}):\n",
        "  \"\"\"\n",
        "    Run a basic experiment to generate answers and evaluate them across datasets.\n",
        "\n",
        "    Args:\n",
        "        helper (Llama7BHelper): An instance of the Llama7BHelper class.\n",
        "        sampled_datasets (dict): A dictionary of sampled datasets.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing generated predictions/answers for each dataset.\n",
        "    \"\"\"\n",
        "\n",
        "  # Initialize a dictionary to store generated answers\n",
        "  generated_answers = {}\n",
        "\n",
        "  for dataset_name, dataset in sampled_datasets.items():\n",
        "      print(f\"\\nGenerating answers for {dataset_name}...\")\n",
        "      if dataset_name == 'squad':\n",
        "          questions = dataset['question']\n",
        "      elif dataset_name == 'hotpotqa':\n",
        "          questions = dataset['question']\n",
        "      elif dataset_name == 'triviaqa':\n",
        "          questions = dataset['question']\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "      answers = generate_answers(model=wrapped_model.model, tokenizer=wrapped_model.tokenizer, questions=questions)\n",
        "      generated_answers[dataset_name] = answers\n",
        "  return generated_answers"
      ],
      "metadata": {
        "id": "N7lJkW_kZQ0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questionsX = smaller_datasets[\"triviaqa\"][\"question\"]\n",
        "print(questionsX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r17I04ALZrkX",
        "outputId": "4ef35dad-f95d-4a39-b100-26dedc6901a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What type of dance shoe has a specially hardened sole or attached metal plates?', 'The songs I got Life and Aquarius come from which musical?', 'The name of which musical instrument comes from the German for bells and play?', 'Who narrated the BBC television production of Paddington?', \"What is the name of Hamlet's mother in the Shakespeare play of the same name?\", 'Who played Dr Richard Kimble in the 1993 film The Fugitive?', 'An analgesic drug is commonly called a?', 'In the Blandings Castle stories by P G Wodehouse, what kind of animal is The Empress of Blandings?', \"After 23 years on the air, America's Most Wanted will be winding down. Who has been the host of the series since its inception?\", 'Which club won the Scottish league cup three times in the first eight years of the competition?', \"Which American resort is nicknamed 'Sin City'?\", 'The voice of Bugs Bunny, Daffy Duck, Porky Pig, Sylvester the Cat, Tweety Bird, Foghorn Leghorn, Yosemite Sam, Wile E. Coyote, Woody Woodpecker, Barney Rubble, Mr. Spacely, Speed Buggy, Captain Caveman, Heathcliff, Speedy Gonzales, Elmer Fudd and hundreds of others, which American voice actor was born on May 30, 1908?', 'On this day, April 17th, in 1984, during an infamous incident in London, the murder of which person toolk place which has had long term political implications?', 'The particle physics unit of reactionary particle decay is?', 'Whose autobiography was titled 1966 And All That', 'What sporting surface is 78 feet long and 27 feet wide for singles matches?', 'The Antarctica Treaty that is presently in force ends in which year?', 'Where is the European Court of Human Rights?', 'Ecuador has a border with Peru and which other country ?', 'What was the first national park in the USA', 'Which US State is nicknamed The Beaver State?', 'What paraffin-derived clear, transparent liquid developed in 1924 by W. J. Stoddard and Lloyd E. Jackson began to be used by dry cleaners in 1928 and has become a common organic solvent used in painting and decorating?', 'Which 19th century king sired ten illegitimate children, by the Irish comedienne, Dorothea Jordan?', 'A particular song was copyrighted in 1935 by the Summy Company, crediting authors Preston Ware Orem and Mrs. R.R. Forman. The company was purchased in 1990 by Warner Chappell for US$15 million, with the value of the song estimated at US$5 million. What is the song?', 'Composer Evangelos Odysseas Papathanassiou is better known by what name?', \"Who designed the Queen's wedding dress?\", 'Who painted Girl with a Pearl Earring?', \"What shape is something that is 'Cordiform'?\", 'Titan is a satellite of which planet?', 'Which group released their third studio album X+Y in 2005?', 'When was the sculpture of the four faces on Mount Rushmore begun?', 'Which city is the destination of the yacht race which leaves Sydney every Boxing Day?', 'Which chemical compound is the active ingredient in agricultural lime?', 'Who is the richest resident of Metropolis?', 'Carpology is the study of what?', 'What pattern would be on material described as Tattersall', \"Who wrote the original novel 'The Phantom of the Opera' in 1910?\", 'In the DC Comics world, he started as Dick Grayson and ended up as the leader of the Teen Titans, under the identity of Nightwing. By what name is he more famously know?', 'The 1981 film Chariots of Fire was based on the stories of which two British athletes?', 'No dragons were harmed in the making of this movie is in the credits of which film?', 'What is the flavouring of the liqueur Amaretto?', 'Vancouver is not the first Canadian city to host the winter Olympics. What city holds that honor when they hosted the 1988 games?', 'November 10, 1775 at Tun Tavern, Philadelphia, saw the official birth of branch of the US armed forces?', 'What does the DSM-IV define as: \\xa0\\xa0\\xa0\\xa0A. Over a period of at least 6 months, recurrent, intense sexually arousing fantasies, sexual urges, or behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity. B. The person has acted on these urges, or the sexual urges or fantasies cause marked distress or interpersonal difficulty.', \"What is Mel Gibson's middle name?\", \"The Battle of Asculum took place in 279 BC between the Romans and the Greeks in which the Romans lost 6,000 men, while the Greeks lost 3,500, including many of their officers. Who commanded the 'victorious' Greeks?\", 'What name specifically describes an inhabitant of Dundee?', \"Revealed in the author's archives given to Oxford's Bodelian Library in 2008, what occupationally titled novel had the working name The Reluctant Autumn of George Smiley?\", \"What is Canada's most populous province?\", 'Name either of the Sovereign Base Areas of the UK on Cyprus?', 'What is the English translation of the place name Las Vegas?', \"Which crop that was of great nutritional importance in pre-Columbian Andean civilizations, and which has now entered the mainstream world market, was called as the 'mother of all grains' by the Incas?\", 'In the tv sitcom Allo Allo!, what was Renes surname?', 'Little Boy and Fat Man were US codes for what during World War II?', 'What is the medical description of the loss of hair from the head or body, sometimes to the extent of baldness?', 'Which European countrys national rail network is known as RENFE?', '\"The American film made at the very start of the sound film era, \"\"All Quiet on the Western Front\"\" (1930), on the grimness of warfare in WWI was banned in Germany (after a brief run in 1930) not unexpectedly, but also where?\"', 'Who had an album called Physcomodo?', 'Swede Carl Linnaeus, 1707-78, considered the father of taxonomy of living things, is the lectotype (example specimen) for which species?', 'What was the five-letter surname of the Major, one of the permanent residents?', 'What name is given to the syndrome which involves a delusion that the affected person can transform or has transformed into an animal?', 'In the UK, how much does it cost to buy Trafalgar Square on a Monopoly board?', 'What is thalassophobia a fear of?', 'The island of Tasmania is governed by which country?', 'Denis Gabor won the 1971 Nobel Prize in Physics for which invention?', 'Which late American musician, who pioneered the styles of rock and roll and rockabilly, was born Vincent Eugene Craddock in 1935?', 'The People Party, founded in Coventry in 1972, changed its name in 1975 and again in 1985. The party is represented in Parliament, the Lords and the European Parliament, how is it now known?', 'Who won the 2007 Nobel Peace Prize for his environmental work including the controversial film An Inconvenient Truth?', 'What was the name of the ship that took the Pilgrim Fathers to America?', 'Which is the only English queen never to have set foot on English soil?', \"What is the name of the Mafia's vow of silence?\", \"Which novel features the scholar 'Humbert Humbert'?\", 'Named after a town in Lincolnshire, what is a more common name for bath metal?', 'In the USA, the Tav HaYosher is a certification mark offered to establishments that do what?', \"How many books in the Bible's Old Testament are included in the Catholic version but not in the Protestant one?\", 'In which modern-day country is the birthplace of Buddha (Siddhartha Gautama)?', 'How often does a national census take place in Britain?', 'Which Verdi opera is based on La Dame aux Camellias by Alexandre Dumas fils?', 'What was made and repaired by a Wainwright?', \"What is the name of Jay-Z and Beyonce's daughter?\", 'Which Bradford car manufacturer, up until 1954, built the Javelin and Jupiter models?', 'True or False: Sharks do not blink?', 'The bitter 19th century rivalry between paleontologists Edward Drinker Cope and Othniel Charles Marsh during their search for dinosaur fossils is usually described with what 2-words?', 'Which was the first US state to secede from the Union in December 1860; the first action of the Civil War took place in that state in April the following year?', \"Which author has been nominated for the Booker Prize for the novels 'Briefing For A Descent Into Hell', 'The Sirian Experiments' and 'The Good Terrorist' but is yet to win the award?\", 'Name the first monarch of the House of Lancaster?', 'Which British artists works include The First Real Target?', \"What product invented by Frederick Walton in 1864 became so widely used and 'stepped on' that the name became generic just 14 years after its invention?\", 'Which language - originally a form of Anglo-Chinese jargon was used by traders and businessmen on the China coast?', 'What colour is the bottom stripe on the U.S. flag?', \"Can you name the singer of the title track for '1971 - Diamonds Are Forever'?\", 'Which company advertises it \"gets the red out\"?', 'The navy from which country destroyed the Russian fleet at Port Arthur after a number of attacks in 1904 and 1905?', 'What was the name of Catherine Zeta Jones character in The Darling Buds of May', 'To which family of birds does the linnet belong?', 'What is the surname of the TV sibling characters Chris, Meg and Stewie (and pet Brian)?', 'Which cricketer holds the record of taking the most wickets in Test Matches for England?', 'In what French region would you find Omaha, Juno, and Gold beaches?', 'What is the name of a calculating machine that originally consisted of beans or stones moved in grooves in sand or on tablets of wood, stone, or metal?', 'Give a year in the life of St Ignatius Loyola, founder of the Jesuits.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_datasets['squad'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQy--xg33FM5",
        "outputId": "4f8ee6b0-8cdc-4a36-8895-67d127a7e52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733ea04d058e614000b6595',\n",
              " 'title': 'French_and_Indian_War',\n",
              " 'context': \"In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Cloron had mapped out four years earlier, but where Cloron had limited the record of French claims to the burial of lead plates, Marin constructed and garrisoned forts. He first constructed Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore. He had a road built to the headwaters of LeBoeuf Creek. Marin constructed a second fort at Fort Le Boeuf (present-day Waterford, Pennsylvania), designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson, a chief of the Mingo, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French (whom he accused of killing and eating his father). Traveling to Fort Le Boeuf, he threatened the French with military action, which Marin contemptuously dismissed.\",\n",
              " 'question': 'Where did Marin build first fort?',\n",
              " 'answers': {'answer_start': [425, 425, 444, 425, 444],\n",
              "  'text': ['Fort Presque Isle (near present-day Erie, Pennsylvania',\n",
              "   'Fort Presque Isle',\n",
              "   'near present-day Erie, Pennsylvania',\n",
              "   'Fort Presque Isle',\n",
              "   'near present-day Erie, Pennsylvania']}}"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_datasets['triviaqa'][0]['answer']['aliases']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uw55z9z07Qu",
        "outputId": "a1bbf2d5-e2ea-41a2-cf2b-b04b64fbb7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tap Dance',\n",
              " 'Tapdance',\n",
              " 'Soft shoe dance',\n",
              " 'Tap danced',\n",
              " 'Softshoe',\n",
              " 'Tap shoe',\n",
              " 'Hoofers',\n",
              " 'Tap-dance',\n",
              " 'Tapdancing',\n",
              " 'Tap dancers',\n",
              " 'Tap dancer',\n",
              " 'Tap-dancing',\n",
              " 'Tap dancing',\n",
              " 'Hoofer',\n",
              " 'Tap Dancing',\n",
              " 'Heel clicking',\n",
              " 'Soft shoe',\n",
              " 'Tap (dance)',\n",
              " 'Tap dance']"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evalute Basic Experiment"
      ],
      "metadata": {
        "id": "S4RF-OZd1GWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oprJXCUzi4eu"
      },
      "outputs": [],
      "source": [
        "def evaluate_basic_experiment(sampled_datasets={}, generated_answers={}):\n",
        "  \"\"\"\n",
        "    Run a basic experiment to generate answers and evaluate them across datasets.\n",
        "\n",
        "    Args:\n",
        "        helper (Llama7BHelper): An instance of the Llama7BHelper class.\n",
        "        sampled_datasets (dict): A dictionary of sampled datasets.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results for each dataset.\n",
        "    \"\"\"\n",
        "\n",
        "  # Initialize a dictionary to store evaluation results\n",
        "  evaluation_results = {}\n",
        "\n",
        "  for dataset_name, answers in generated_answers.items():\n",
        "      print(f\"\\nEvaluating answers for {dataset_name}...\")\n",
        "      if dataset_name == 'squad':\n",
        "        references = [sample[\"answers\"]['text'] for sample in sampled_datasets[\"squad\"]]\n",
        "      elif dataset_name == 'hotpotqa':\n",
        "        references = [[sample[\"answer\"]] for sample in sampled_datasets[\"hotpotqa\"]]\n",
        "      elif dataset_name == 'triviaqa':\n",
        "        references = [sample[\"answer\"]['aliases'] for sample in sampled_datasets[\"triviaqa\"]]\n",
        "            #what is the color of the sky\n",
        "            #I say: blue\n",
        "            #triviqa: \"blue, BLUE, blUE, bleu, cyan, aqua\", etc.\n",
        "      else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "      # Evaluate answers\n",
        "      metrics = evaluate_answers(answers, references)\n",
        "      # Interpret metrics\n",
        "      # interpreted_metrics = interpret_metrics(metrics)\n",
        "      evaluation_results[dataset_name] = metrics\n",
        "\n",
        "      # Display interpreted metrics\n",
        "      # for metric, details in interpreted_metrics.items():\n",
        "      #     print(f\"{metric.upper()}: {details['score']} - {details['description']}\")\n",
        "  return evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first 5 rows of each dataset\n",
        "tiny_datasets = {\n",
        "    dataset_name: dataset.select(range(10))  # Select the first 5 rows\n",
        "    for dataset_name, dataset in smaller_datasets.items()\n",
        "}\n",
        "\n",
        "# Verify the result\n",
        "for name, data in tiny_datasets.items():\n",
        "    print(f\"Dataset: {name}, Number of Examples: {data.num_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjrpPcA1bMUZ",
        "outputId": "bd5e053b-4bf2-469c-9627-87fbbf928e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: squad, Number of Examples: 10\n",
            "Dataset: hotpotqa, Number of Examples: 10\n",
            "Dataset: triviaqa, Number of Examples: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tiny_datasets[\"hotpotqa\"][:1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pTwChzhbdF-",
        "outputId": "a0aa0e4a-f2f1-4af4-c653-7aafda8dc1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': ['5add1d575542992c1e3a2540'], 'question': [\"What nationality was Oliver Reed's character in the film Royal Flash?\"], 'answer': ['Prussian'], 'type': ['bridge'], 'level': ['hard'], 'supporting_facts': [{'sent_id': [0, 2, 0], 'title': ['Royal Flash (film)', 'Royal Flash (film)', 'Otto von Bismarck']}], 'context': [{'sentences': [[\"Robin Barton (born 5 November 1958) is a British art dealer dealing primarily with Banksy's.\", ' Barton studied photography and graphic design at the Exeter College of Art and Design and this was his first encounter with Russell Young.', ' Moving to London in 1980 he began working as a freelance photographer for music and fashion publications \"Sounds\", \"NME\", \"Blitz\", \"The Face\" moving on to working regularly for pioneering \"Independent Magazine\" photographing amongst others Sir Alec Guinness, Oliver Reed, Johnny Depp, Lou Reed, Hugh Grant and Sir Peter Hall.', ' Laterly he worked for other publications \"Sunday Times\", \"Sunday Telegraph\", \"Elle\", \"Vogue\", \"Tatler\" and \"Blueprint\".'], ['Funny Bones is a 1995 British-American comedy-drama film from Hollywood Pictures.', ' It was written, directed and produced by Peter Chelsom, co produced by Simon Fields, and co written by Peter Flannery.', ' The music score was by John Altman, and the cinematography by Eduardo Serra.', ' Set in Las Vegas and Blackpool, England, the film stars Oliver Platt, Jerry Lewis, Lee Evans, Leslie Caron, Richard Griffiths, Sadie Corre, Oliver Reed, George Carl, Freddie Davies and Ian McNeice.'], ['Ivan Dragomiloff is a fictional character, the chairman of \"The Assassination Bureau, Ltd\" in the book of that name by Jack London.', ' The character was played by actor Oliver Reed in the film of the same name.'], ['Robert Oliver Reed (13 February 1938\\xa0 2 May 1999) was an English actor known for his upper-middle class, macho image, hellraiser lifestyle, and \"tough guy\" roles.', ' Notable films include \"The Trap\" (1966), \"Oliver!', '\" (1968), \"Women in Love\" (1969), \"Hannibal Brooks\" (1969), \"The Devils\" (1971), \"The Three Musketeers\" (1973), \"Tommy\" (1975), \"Lion of the Desert\" (1981), \"Castaway\" (1986), \"The Adventures of Baron Munchausen\" (1988) and \"Funny Bones\" (1995).', ' For \"Gladiator\" (2000), his final film, Reed was posthumously nominated for the BAFTA Award for Best Actor in a Supporting Role.'], ['Sir Harry Paget Flashman is a fictional character created by Thomas Hughes (18221896) in a semi-autobiographical \"Tom Brown\\'s School Days\" (1857) and later developed by George MacDonald Fraser (19252008).', ' Harry Flashman appears in a series of 12 of Fraser\\'s books, collectively known as \"The Flashman Papers\", with covers illustrated by Arthur Barbosa.', ' Flashman was played by Malcolm McDowell in the Richard Lester 1975 film \"Royal Flash\".'], ['Royal Flash is a 1975 film based on George MacDonald Fraser\\'s second Flashman novel, \"Royal Flash\".', ' It stars Malcolm McDowell as Flashman.', ' Additionally, Oliver Reed appeared in the role of Otto von Bismarck, Alan Bates as Rudi von Sternberg, and Florinda Bolkan played Lola Montez.', ' Fraser wrote the screenplay and the film was directed by Richard Lester.'], ['Royal Flash is a 1970 novel by George MacDonald Fraser.', ' It is the second of the Flashman novels.', ' It was made into the film \"Royal Flash\" in 1975 and remains the only Flashman novel to be filmed.'], ['The Duke of Hamilton was one of the oldest pubs in London, situated in Hampstead.', \" It was a popular meeting place for actors Peter O'Toole, Oliver Reed and Richard Burton.\", ' Reed would be seen for long periods at the pub on a daily basis.'], ['Lion of the Desert is a 1981 Libyan historical action film starring Anthony Quinn as Libyan tribal leader Omar Mukhtar, a Bedouin leader fighting the \"Regio Esercito\" (Italian Royal Army) in the years leading up to World War II, and Oliver Reed as Italian General Rodolfo Graziani, who attempted to defeat Mukhtar.', ' It was directed by Moustapha Akkad and funded by the government under Colonel Muammar Gaddafi.', ' Released in May 1981, the film was liked by critics and audiences but performed poorly financially, bringing in just $1 million net worldwide.', ' .', ' The film was banned in Italy in 1982 and was only shown on pay TV in 2009.'], ['Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg (1 April 1815  30 July 1898), known as Otto von Bismarck (] ), was a conservative Prussian statesman who dominated German and European affairs from the 1860s until 1890.', ' In the 1860s, he engineered a series of wars that unified the German states, deliberately excluding Austria, into a powerful German Empire under Prussian leadership.', \" With that accomplished by 1871, he skillfully used balance of power diplomacy to maintain Germany's position in a Europe which, despite many disputes and war scares, remained at peace.\", ' For historian Eric Hobsbawm, it was Bismarck who \"remained undisputed world champion at the game of multilateral diplomatic chess for almost twenty years after 1871, [and] devoted himself exclusively, and successfully, to maintaining peace between the powers\".', ' However, his annexation of Alsace-Lorraine gave new fuel to French nationalism and promoted Germanophobia in France.', ' This helped set the stage for the First World War.']], 'title': ['Robin Barton', 'Funny Bones', 'Ivan Dragomiloff', 'Oliver Reed', 'Harry Flashman', 'Royal Flash (film)', 'Royal Flash', 'The Duke of Hamilton', 'Lion of the Desert', 'Otto von Bismarck']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPBln21H7UDY"
      },
      "source": [
        "### Execute Experiment & Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # print(tiny_datasets[\"triviaqa\"][:5])\n",
        "# counter = 0\n",
        "# for idx, sample in enumerate(tiny_datasets[\"triviaqa\"]):\n",
        "#   if counter <1:\n",
        "#     print(f\"Sample {idx + 1}:\")\n",
        "#     print(sample.keys())\n",
        "#     print(f\"Question: {sample['question']}\")\n",
        "\n",
        "#     answers = sample.get(\"answer\", {})\n",
        "#     print(answers)\n",
        "#     print(sample[\"answer\"].keys())\n",
        "#     counter+=1\n"
      ],
      "metadata": {
        "id": "B8UGLcMvcYFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answers = run_basic_experiment(helper, tiny_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "41c5f41b6fa24798abf8e7da36203c07",
            "a6092bbfea6f4db08af921ebbfa7e98f",
            "ec421589b001441dabdb0b00fa6049ca",
            "c219e683df3d42d391acd46c88c39220",
            "a505df621ebb44bd90f5fb6ffeba879f",
            "7708a30cb0714b9bbba7680794887f98",
            "c1c9d9f3fab94cedbe92516c984b82e0",
            "b563ef720f804118a11e39e9f0768137",
            "40f324c34d5f4910a99f3a67e482f871",
            "73a54247ac5a4d7088be78d5fb6472d2",
            "12fc07e9e5ad40f5926101137ea00fa3",
            "83352f984ed84117a245f01cece8967a",
            "b6b1ed4782ad48afa530b62766dffe86",
            "5ea61eb00fe14d04a85f3005e3214dd1",
            "6a9ff546647d4756b83fb81f10fd757e",
            "9251ac0a35694e42ba479e520b4d85ee",
            "f9e5ccb5d9f747bfad4df4a99006e8b0",
            "460fb0b095174cbebd898c6255d999f5",
            "9fed4563bbef4014854978d7c78ad0fb",
            "91eb99195b7042e7b1fa2c15f20f4963",
            "ecd73d7029ab419a8542556d5bb2a3a7",
            "510cc3e139d941139b8a13a0fd4abf54",
            "fb62a05a4bac4bba85a5dc2f39dcdb4e",
            "7c27a065b89048d2ae72ba00194f842a",
            "8cc5a5d4c1bc41d3bb0bb2b5a0af0ff1",
            "2e2c33f6f2f04911a9796785616fbf66",
            "be19af89d47141308a990c0b90b20e44",
            "16939cec9cba46f2b82775afdc3f8f46",
            "16e000b1b6174ba2ba6eb6c0f196c554",
            "1533926d469242e9af1117b29cdd2a8a",
            "5cfcb336234a4af49864ad99543bd9a6",
            "3041f9118a104af095d23416ef1f963d",
            "40ee16364c9e47e48acb5efb1bf4a129"
          ]
        },
        "id": "bXpv8cgJa0ui",
        "outputId": "91933154-b1fe-4e23-9d1f-48466d045f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating answers for squad...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Answers:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41c5f41b6fa24798abf8e7da36203c07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating answers for hotpotqa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Answers:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83352f984ed84117a245f01cece8967a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating answers for triviaqa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Answers:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb62a05a4bac4bba85a5dc2f39dcdb4e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generated_answers.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Je-voPCtPAK",
        "outputId": "b537b7d8-5aba-46b3-b2a4-9d65bffa22f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['squad', 'hotpotqa', 'triviaqa'])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generated_answers[\"squad\"]\n",
        "# for k in generated_answers[\"squad\"].items():\n",
        "\n",
        "#   print(f\"This is the predicted answer: {k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DrnxaXps-VD",
        "outputId": "ea910c43-5f54-4cde-8f35-e33fc753c2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['About the beginning of the eighteenth century, the Spanish and Indians established settlements in the area. It is believed that some of the earliest settlers were Indians from Sonoma who came from Sonoma to Point Reyes, a distance of more than 30 miles.\\nQuestion',\n",
              " 'When the net force is zero, there must be equal and opposite reaction forces on every body involved to make the vector',\n",
              " 'India was the country Thoreau dreamed of visiting and he never got a chance to. So, its fair to say, Thoreau would write something that would describe the India that he dreamed of it being. He probably would be very critical of the way India exists now and would be angry at how industrialization (and globalization, particularly) has taken away the culture of India. But the India that he would have dreamed of and wrote about, would have to',\n",
              " 'A Papal Bull\\nQuestion: In 1809, Napoleon tried to impose a state of sie',\n",
              " 'The U.S. government provides social services in the form of basic medical care and education, which are largely available to']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_datasets['squad'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBGJOU1d4yMh",
        "outputId": "694cb94c-25cf-4cee-f2cf-729b6c88d3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5ad26a5fd7d075001a42931b',\n",
              " 'title': 'Force',\n",
              " 'context': 'Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force exactly balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.',\n",
              " 'question': 'What increases or decreases in response to static friction?',\n",
              " 'answers': {'answer_start': [], 'text': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1XCZNa07ZZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "b9bfd150-4bd3-45c4-9133-5b2bcf6f660b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating answers for squad...\n",
            "['Marin built a small fort with a palisade and a few huts to protect the shipbuilding shipyards.', \"The magnitude of the static friction always increases when you decrease the shear stress. The magnitude of the static friction always decreases when you decrease the normal load .\\nThe friction between two surfaces is called static friction. We need to determine the coefficient of static friction () of two bodies. We can write the law of static friction with the help of Newton's second law .\\nThe maximum limit of the normal force we can apply for a body is called the\", 'I know of no more encouraging fact than the unquestioned ability of man to elevate his life by conscious endeavor.\\nThis quote encapsulates a great deal about the philosophy behind the idea of Consciousness and Social Transformation program, that I am privilege to be a part of at the Satyagraha Institute in Pune, India.\\nAfter a series of discussions and interactions, it has been decided that there are 7 principles underlying', 'The Spanish American War\\nQuestion', 'High tax and spending, particularly on the welfare state. (Social spending is also higher in many other democracies, but they havent reduced inequality as much as we have.)\\nComment: Its not obvious why taxing and redistribution are required. Why isnt that just a market outcome in the absence of government? Isnt the market the most rational thing around? And why doesnt lower income inequality in those other countries indicate', 'In the early 90s, the legal systems of many developing countries were re-engineered to include a formal legal ownership registration system, where land or housing can be registered through the courts. This system enables the government to establish a property title, a claim to have exclusive rights over a particular piece of land within the legal framework of a country.\\nThe formal legal system has also had multiple adverse effects on the livelihoods of poor households and on the overall liv', 'He came', 'When certain conditions change.\\nGold can be formed in all kinds of environments, the difference is how the gold is deposited.\\nGold can be deposited by erosion. Gold can be deposited by a volcano. Gold can be deposited by meteorites. Gold can be deposited by plate tectonics. Gold can be deposited in the earth by tectonic processes that bring it to the surface like volcanic, intrusive (magma', 'The Suez Canal and the Gulf of Aqaba. When it was built in 1869, this canal was one of the first projects undertaken by the French to gain a greater stake in the region. By building this channel, France was able to gain direct access to the Suez Canal to Egypt. This created more interest in the region. As the result of the completion of the Suez Canal, Great Britain was more interested in the region. British troops were station', 'The use of the sea lamprey parasite Entospecimencephalus was thought to limit the life of the ctenophore [7]. But, in the experiment, entospecimencephalus larvae did not persist after a year or so. The only success was the parasitic copepod Tigriopus cf. grandicornis 18 years ago, but, again, after a few years, the copepod population became']\n",
            "[['Fort Presque Isle (near present-day Erie, Pennsylvania', 'Fort Presque Isle', 'near present-day Erie, Pennsylvania', 'Fort Presque Isle', 'near present-day Erie, Pennsylvania'], [], [], [], [], [], [], [], ['Pacific Ocean', 'Pacific Ocean', 'Pacific Ocean'], ['introduction of Beroe', 'accidental introduction of Beroe']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "attempt to get argmax of an empty sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-2d0af750ca19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_basic_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiny_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Access evaluation results for each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFinal Metrics for {dataset_name}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-141-22a32e7a0376>\u001b[0m in \u001b[0;36mevaluate_basic_experiment\u001b[0;34m(sampled_datasets, generated_answers)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# Evaluate answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0;31m# Interpret metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# interpreted_metrics = interpret_metrics(metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-3b7cb49eca2c>\u001b[0m in \u001b[0;36mevaluate_answers\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute ROUGE scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mrouge_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_stemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Compute BLEU scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886/rouge.py\u001b[0m in \u001b[0;36m_compute\u001b[0;34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmulti_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rouge_score/rouge_scorer.py\u001b[0m in \u001b[0;36mscore_multi\u001b[0;34m(self, targets, prediction)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_dicts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0mmax_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \"\"\"\n\u001b[1;32m   1228\u001b[0m     \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ],
      "source": [
        "evaluation_results = evaluate_basic_experiment(tiny_datasets, generated_answers)\n",
        "\n",
        "# Access evaluation results for each dataset\n",
        "for dataset_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\nFinal Metrics for {dataset_name}:\")\n",
        "    for metric, details in metrics.items():\n",
        "        print(f\"{metric.upper()}: {details['score']} - {details['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset_name, metrics in evaluation_results.items():\n",
        "  print(dataset_name, metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0lZu3pSirfZ",
        "outputId": "e5016f97-aaad-4322-9a7a-520ffe7ff3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "squad {'rouge1': {'score': 0.0045, 'description': 'Low content coverage.'}, 'rouge2': {'score': 0.0, 'description': 'Low phrase overlap.'}, 'rougel': {'score': 0.0045, 'description': 'No interpretation available.'}, 'bleu': {'score': 0.0, 'description': 'No interpretation available.'}, 'bertscore_precision': {'score': 0.1606, 'description': 'No interpretation available.'}, 'bertscore_recall': {'score': 0.1302, 'description': 'No interpretation available.'}, 'bertscore_f1': {'score': 0.1438, 'description': 'Weak semantic alignment.'}, 'sts': {'score': 0.0012, 'description': 'Poor semantic coherence.'}}\n",
            "hotpotqa {'rouge1': {'score': 0.0051, 'description': 'Low content coverage.'}, 'rouge2': {'score': 0.0, 'description': 'Low phrase overlap.'}, 'rougel': {'score': 0.0051, 'description': 'No interpretation available.'}, 'bleu': {'score': 0.0, 'description': 'No interpretation available.'}, 'bertscore_precision': {'score': 0.7868, 'description': 'No interpretation available.'}, 'bertscore_recall': {'score': 0.6933, 'description': 'No interpretation available.'}, 'bertscore_f1': {'score': 0.7348, 'description': 'Good semantic alignment.'}, 'sts': {'score': 0.0424, 'description': 'Poor semantic coherence.'}}\n",
            "triviaqa {'rouge1': {'score': 0.0, 'description': 'Low content coverage.'}, 'rouge2': {'score': 0.0, 'description': 'Low phrase overlap.'}, 'rougel': {'score': 0.0, 'description': 'No interpretation available.'}, 'bleu': {'score': 0.0, 'description': 'No interpretation available.'}, 'bertscore_precision': {'score': 0.0, 'description': 'No interpretation available.'}, 'bertscore_recall': {'score': 0.0, 'description': 'No interpretation available.'}, 'bertscore_f1': {'score': 0.0, 'description': 'Weak semantic alignment.'}, 'sts': {'score': 0.0486, 'description': 'Poor semantic coherence.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb7AUwOQ7G8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "eaf52ae3-6f95-4574-f63b-bea343d02ac2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"The following id_vars or value_vars are not present in the DataFrame: ['rouge1', 'rouge2', 'rougeL', 'bleu', 'BERTScore_F1', 'STS']\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-8343898e1ac1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"squad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-002108b21dcd>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(results, metrics_to_plot)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Melt the dataframe for easier plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf_melted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_to_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Metric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m   9940\u001b[0m         \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9941\u001b[0m     ) -> DataFrame:\n\u001b[0;32m-> 9942\u001b[0;31m         return melt(\n\u001b[0m\u001b[1;32m   9943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9944\u001b[0m             \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/melt.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mlab\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_found\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnot_found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             ]\n\u001b[0;32m---> 74\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;34m\"The following id_vars or value_vars are not present in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;34mf\"the DataFrame: {missing_labels}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"The following id_vars or value_vars are not present in the DataFrame: ['rouge1', 'rouge2', 'rougeL', 'bleu', 'BERTScore_F1', 'STS']\""
          ]
        }
      ],
      "source": [
        "plot_metrics(evaluation_results[\"squad\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo32dHSJkDMe"
      },
      "outputs": [],
      "source": [
        "save_evaluation_results(evaluation_results, save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og881oKqlNHZ"
      },
      "source": [
        "### Run Greedy Decoding Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8qIsNRtlNHa"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to store generated answers\n",
        "generated_answers = {}\n",
        "\n",
        "for dataset_name, dataset in sampled_datasets.items():\n",
        "    print(f\"\\nGenerating answers for {dataset_name}...\")\n",
        "    if dataset_name == 'squad':\n",
        "        questions = dataset['question']\n",
        "    elif dataset_name == 'hotpotqa':\n",
        "        questions = dataset['question']\n",
        "    elif dataset_name == 'triviaqa':\n",
        "        questions = [item['query'] for item in dataset['question']]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    answers = generate_answers(model, tokenizer, questions, do_sample=False) #setting to False for greedy\n",
        "    generated_answers[dataset_name] = answers\n",
        "\n",
        "\n",
        "# Initialize a dictionary to store evaluation results\n",
        "evaluation_results = {}\n",
        "\n",
        "for dataset_name, answers in generated_answers.items():\n",
        "    print(f\"\\nEvaluating answers for {dataset_name}...\")\n",
        "    if dataset_name == 'squad':\n",
        "        references = sampled_datasets['squad']['answers']['text']\n",
        "    elif dataset_name == 'hotpotqa':\n",
        "        references = sampled_datasets['hotpotqa']['answer']\n",
        "    elif dataset_name == 'triviaqa':\n",
        "        references = [item['answer']['value'][0] if isinstance(item['answer']['value'], list) else item['answer']['value'] for item in sampled_datasets['triviaqa']]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # Evaluate answers\n",
        "    metrics = evaluate_answers(answers, references)\n",
        "    # Interpret metrics\n",
        "    interpreted_metrics = interpret_metrics(metrics)\n",
        "    evaluation_results[dataset_name] = interpreted_metrics\n",
        "\n",
        "    # Display interpreted metrics\n",
        "    for metric, details in interpreted.items():\n",
        "        print(f\"{metric.upper()}: {details['score']} - {details['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1k1WBa8lNHa"
      },
      "outputs": [],
      "source": [
        "plot_evaluation_metrics(evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktIFbka0lNHa"
      },
      "outputs": [],
      "source": [
        "save_evaluation_results(evaluation_results, save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbNPrPhpmGak"
      },
      "source": [
        "## Run Beam Search Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDhng2I3lMgF"
      },
      "outputs": [],
      "source": [
        "# Process each dataset and evaluate with beam search\n",
        "results = {}\n",
        "beam_width = 5  # Adjust based on your computational resources\n",
        "\n",
        "for name in dataset_names:\n",
        "    if name in datasets_loaded:\n",
        "        print(f\"\\nStarting evaluation for {name} with beam width = {beam_width}...\")\n",
        "        metrics = evaluate_manual_beam_search(\n",
        "            helper=helper,\n",
        "            dataset=datasets_loaded[name],\n",
        "            max_length=100,\n",
        "            num_beams=beam_width\n",
        "        )\n",
        "        results[name] = metrics\n",
        "save_evaluation_results(results, save_directory, filename='llama2_7b_qa_semantic_coherence_metrics.csv')\n",
        "\n",
        "plot_metrics(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0dA6KtlYS6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE7-AHJ-Ma7p"
      },
      "source": [
        "## Running experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWNc6pJfMXdV"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "helper = Llama7BHelper(pretrained_model=\"huggyllama/llama-7b\")\n",
        "prompt = \"Explain the theory of relativity in simple terms.\"\n",
        "generated_text = manual_beam_search(\n",
        "    helper=helper,\n",
        "    prompt=prompt,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    early_stopping=True,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "print(\"Generated Text:\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nAxpYPA5ch3"
      },
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4oAijcp5b1f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "NEED TO CALL FUNCTION TO GENERATE STILL\n",
        "generate baseline performance\n",
        "\n",
        "evaluate semantic coherence\n",
        "\n",
        "generate beam search baseline\n",
        "\n",
        "activation baseline\n",
        "\n",
        "\n",
        "# Interpret the metrics dynamically\n",
        "interpreted_metrics_dynamic = interpret_metrics_dynamic(evaluation_scores)\n",
        "\n",
        "# Display the results\n",
        "for metric, details in interpreted_metrics_dynamic.items():\n",
        "    print(f\"{metric.upper()}: {details['score']} - {details['description']}\")\n",
        "\n",
        "Nina does: https://github.com/nrimsky/LM-exp/blob/main/steering/activation_engineering.ipynb\n",
        "return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "--> should I be doing batch_decode?\n",
        "\n",
        "\n",
        "Using up GPU memory\n",
        "https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530\n",
        "try clearing it\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FREu52JWwIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q-qtB3uMKyeu",
        "QeuIe1IiLYHO",
        "pcXCewXwMAXf",
        "SKjOH25Dm1a9",
        "Og881oKqlNHZ",
        "kbNPrPhpmGak",
        "uE7-AHJ-Ma7p"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "711f7cc101f34ec2a8b25a5403eb6ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2681e6b5672f4504860a18ce09c6c0de",
              "IPY_MODEL_947547694f304cc7a3edd9157043cfcc",
              "IPY_MODEL_f455cbd2833a4ec2997f47ee4aa20632"
            ],
            "layout": "IPY_MODEL_e2f3be9888164d0e8f5bfa9fcc3b1c66"
          }
        },
        "2681e6b5672f4504860a18ce09c6c0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_152381714d41414d892e82540e6d3010",
            "placeholder": "",
            "style": "IPY_MODEL_40dd58160a194fa4a8128be7db3a6ec3",
            "value": "Downloadingbuilderscript:100%"
          }
        },
        "947547694f304cc7a3edd9157043cfcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c76eab24e1a4c78a9b4f41389c7c231",
            "max": 7950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b9ba6e59de14d2d90c388003a4e4e26",
            "value": 7950
          }
        },
        "f455cbd2833a4ec2997f47ee4aa20632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d87dfdac1a54904a0171f2c8349337b",
            "placeholder": "",
            "style": "IPY_MODEL_1d5131e06cca4ef2bf0a7c39189d9d32",
            "value": "7.95k/7.95k[00:00&lt;00:00,459kB/s]"
          }
        },
        "e2f3be9888164d0e8f5bfa9fcc3b1c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152381714d41414d892e82540e6d3010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40dd58160a194fa4a8128be7db3a6ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c76eab24e1a4c78a9b4f41389c7c231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9ba6e59de14d2d90c388003a4e4e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d87dfdac1a54904a0171f2c8349337b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5131e06cca4ef2bf0a7c39189d9d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e890d4c733884403b728e5c077ddc4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ba61c2ed5444ab988c2533e73dbdd27",
              "IPY_MODEL_71e272a121d84475b06ed8258ac32e62",
              "IPY_MODEL_b8c55e990c824d6cb5ec69f07309f639"
            ],
            "layout": "IPY_MODEL_4e9853e906f940cbbde8c100eb96ac8e"
          }
        },
        "7ba61c2ed5444ab988c2533e73dbdd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_703c06f835a2431d87b0ea2999eb1211",
            "placeholder": "",
            "style": "IPY_MODEL_1b332143fba54714b36c92b419ab60ee",
            "value": "100%"
          }
        },
        "71e272a121d84475b06ed8258ac32e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b621f250eaf4d2ba262ca696931a40e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5256cfa3f714eeaadb8cd880972b8b5",
            "value": 1
          }
        },
        "b8c55e990c824d6cb5ec69f07309f639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7acfdc98a68b48d38ca4f97de1dce367",
            "placeholder": "",
            "style": "IPY_MODEL_10fc5503fc134022a2474aab1ca023a3",
            "value": "1/1[00:00&lt;00:00,30.48it/s]"
          }
        },
        "4e9853e906f940cbbde8c100eb96ac8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703c06f835a2431d87b0ea2999eb1211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b332143fba54714b36c92b419ab60ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b621f250eaf4d2ba262ca696931a40e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5256cfa3f714eeaadb8cd880972b8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7acfdc98a68b48d38ca4f97de1dce367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fc5503fc134022a2474aab1ca023a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5441aee34eec40ea81d07d9eb79cb7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b70300cff0b4bfb9c5f33a2a0702fa1",
              "IPY_MODEL_4d5ac47a8d31497b9c97fb7d94b7d144",
              "IPY_MODEL_9d58ce27c0e54a1fb07d704bf886c38e"
            ],
            "layout": "IPY_MODEL_00317d055ca64965a8a1351a0a24549b"
          }
        },
        "5b70300cff0b4bfb9c5f33a2a0702fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98541c353c1f496ea09f783537d41f5a",
            "placeholder": "",
            "style": "IPY_MODEL_d01f116d2c504bf09745acfc8ec83722",
            "value": "100%"
          }
        },
        "4d5ac47a8d31497b9c97fb7d94b7d144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de531bc98b94b7cb59c82cd9d70071c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b73f62fca0e4a81bab30d5851688a55",
            "value": 1
          }
        },
        "9d58ce27c0e54a1fb07d704bf886c38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc86a221dc574769bc01dea57960f787",
            "placeholder": "",
            "style": "IPY_MODEL_abf55080cd5e4c88b45bf69fca76f2d0",
            "value": "1/1[00:00&lt;00:00,62.00it/s]"
          }
        },
        "00317d055ca64965a8a1351a0a24549b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98541c353c1f496ea09f783537d41f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d01f116d2c504bf09745acfc8ec83722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0de531bc98b94b7cb59c82cd9d70071c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b73f62fca0e4a81bab30d5851688a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc86a221dc574769bc01dea57960f787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abf55080cd5e4c88b45bf69fca76f2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac27e9fccce46a0bfa0ac148af4bf06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_051ed2071efe4d658fddc272ea0255ba",
              "IPY_MODEL_dc9e487b9627407b97f63facd241547d",
              "IPY_MODEL_a463d0eb4faa4d3f99ba9558be4298c3"
            ],
            "layout": "IPY_MODEL_f9360d53ae0343ea85a1866dc91815d5"
          }
        },
        "051ed2071efe4d658fddc272ea0255ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43c0842408a344759d73f1952ab73841",
            "placeholder": "",
            "style": "IPY_MODEL_bc931b0521de4b1c9496465043d31537",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "dc9e487b9627407b97f63facd241547d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c19da03a6f3401697b7a56b4dd6251b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cfd90b935ca47069bc0ff51e12a0d8b",
            "value": 3
          }
        },
        "a463d0eb4faa4d3f99ba9558be4298c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1eceb7e709e41489859cd25c45d6f07",
            "placeholder": "",
            "style": "IPY_MODEL_9587bb20d6754ed2b3618e3d580ea46a",
            "value": "3/3[02:24&lt;00:00,46.47s/it]"
          }
        },
        "f9360d53ae0343ea85a1866dc91815d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c0842408a344759d73f1952ab73841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc931b0521de4b1c9496465043d31537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c19da03a6f3401697b7a56b4dd6251b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cfd90b935ca47069bc0ff51e12a0d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1eceb7e709e41489859cd25c45d6f07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9587bb20d6754ed2b3618e3d580ea46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c5f41b6fa24798abf8e7da36203c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6092bbfea6f4db08af921ebbfa7e98f",
              "IPY_MODEL_ec421589b001441dabdb0b00fa6049ca",
              "IPY_MODEL_c219e683df3d42d391acd46c88c39220"
            ],
            "layout": "IPY_MODEL_a505df621ebb44bd90f5fb6ffeba879f"
          }
        },
        "a6092bbfea6f4db08af921ebbfa7e98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7708a30cb0714b9bbba7680794887f98",
            "placeholder": "",
            "style": "IPY_MODEL_c1c9d9f3fab94cedbe92516c984b82e0",
            "value": "GeneratingAnswers:100%"
          }
        },
        "ec421589b001441dabdb0b00fa6049ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b563ef720f804118a11e39e9f0768137",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40f324c34d5f4910a99f3a67e482f871",
            "value": 10
          }
        },
        "c219e683df3d42d391acd46c88c39220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a54247ac5a4d7088be78d5fb6472d2",
            "placeholder": "",
            "style": "IPY_MODEL_12fc07e9e5ad40f5926101137ea00fa3",
            "value": "10/10[00:28&lt;00:00,3.10s/it]"
          }
        },
        "a505df621ebb44bd90f5fb6ffeba879f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7708a30cb0714b9bbba7680794887f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c9d9f3fab94cedbe92516c984b82e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b563ef720f804118a11e39e9f0768137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f324c34d5f4910a99f3a67e482f871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73a54247ac5a4d7088be78d5fb6472d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12fc07e9e5ad40f5926101137ea00fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83352f984ed84117a245f01cece8967a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6b1ed4782ad48afa530b62766dffe86",
              "IPY_MODEL_5ea61eb00fe14d04a85f3005e3214dd1",
              "IPY_MODEL_6a9ff546647d4756b83fb81f10fd757e"
            ],
            "layout": "IPY_MODEL_9251ac0a35694e42ba479e520b4d85ee"
          }
        },
        "b6b1ed4782ad48afa530b62766dffe86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9e5ccb5d9f747bfad4df4a99006e8b0",
            "placeholder": "",
            "style": "IPY_MODEL_460fb0b095174cbebd898c6255d999f5",
            "value": "GeneratingAnswers:100%"
          }
        },
        "5ea61eb00fe14d04a85f3005e3214dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fed4563bbef4014854978d7c78ad0fb",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91eb99195b7042e7b1fa2c15f20f4963",
            "value": 10
          }
        },
        "6a9ff546647d4756b83fb81f10fd757e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecd73d7029ab419a8542556d5bb2a3a7",
            "placeholder": "",
            "style": "IPY_MODEL_510cc3e139d941139b8a13a0fd4abf54",
            "value": "10/10[00:25&lt;00:00,2.38s/it]"
          }
        },
        "9251ac0a35694e42ba479e520b4d85ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e5ccb5d9f747bfad4df4a99006e8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460fb0b095174cbebd898c6255d999f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fed4563bbef4014854978d7c78ad0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91eb99195b7042e7b1fa2c15f20f4963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecd73d7029ab419a8542556d5bb2a3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "510cc3e139d941139b8a13a0fd4abf54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb62a05a4bac4bba85a5dc2f39dcdb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c27a065b89048d2ae72ba00194f842a",
              "IPY_MODEL_8cc5a5d4c1bc41d3bb0bb2b5a0af0ff1",
              "IPY_MODEL_2e2c33f6f2f04911a9796785616fbf66"
            ],
            "layout": "IPY_MODEL_be19af89d47141308a990c0b90b20e44"
          }
        },
        "7c27a065b89048d2ae72ba00194f842a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16939cec9cba46f2b82775afdc3f8f46",
            "placeholder": "",
            "style": "IPY_MODEL_16e000b1b6174ba2ba6eb6c0f196c554",
            "value": "GeneratingAnswers:100%"
          }
        },
        "8cc5a5d4c1bc41d3bb0bb2b5a0af0ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1533926d469242e9af1117b29cdd2a8a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cfcb336234a4af49864ad99543bd9a6",
            "value": 10
          }
        },
        "2e2c33f6f2f04911a9796785616fbf66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3041f9118a104af095d23416ef1f963d",
            "placeholder": "",
            "style": "IPY_MODEL_40ee16364c9e47e48acb5efb1bf4a129",
            "value": "10/10[00:26&lt;00:00,2.83s/it]"
          }
        },
        "be19af89d47141308a990c0b90b20e44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16939cec9cba46f2b82775afdc3f8f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e000b1b6174ba2ba6eb6c0f196c554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1533926d469242e9af1117b29cdd2a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfcb336234a4af49864ad99543bd9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3041f9118a104af095d23416ef1f963d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ee16364c9e47e48acb5efb1bf4a129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}